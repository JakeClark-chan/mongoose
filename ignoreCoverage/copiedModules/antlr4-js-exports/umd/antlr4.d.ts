export default index_web;
export const __esModule: boolean;
export class ATN {
    constructor(grammarType: any, maxTokenType: any);
    /**
     * Used for runtime deserialization of ATNs from strings
     * The type of the ATN.
    */
    grammarType: any;
    maxTokenType: any;
    states: any[];
    /**
     * Each subrule/rule is a decision point and we must track them so we
     * can go back later and build DFA predictors for them.  This includes
     * all the rules, subrules, optional blocks, ()+, ()* etc...
     */
    decisionToState: any[];
    ruleToStartState: any[];
    ruleToStopState: any;
    modeNameToStartState: {};
    /**
     * For lexer ATNs, this maps the rule index to the resulting token type.
     * For parser ATNs, this maps the rule index to the generated bypass token
     * type if the {@link ATNDeserializationOptions//isGenerateRuleBypassTransitions}
     * deserialization option was specified; otherwise, this is {@code null}
     */
    ruleToTokenType: any;
    /**
     * For lexer ATNs, this is an array of {@link LexerAction} objects which may
     * be referenced by action transitions in the ATN
     */
    lexerActions: any;
    modeToStartState: any[];
    /**
     * Compute the set of valid tokens that can occur starting in state {@code s}.
     * If {@code ctx} is null, the set of tokens will not include what can follow
     * the rule surrounding {@code s}. In other words, the set will be
     * restricted to tokens reachable staying within {@code s}'s rule
     */
    nextTokensInContext(s: any, ctx: any): {
        intervals: any[] | null;
        readOnly: boolean;
        first(v: any): any;
        addOne(v: any): void;
        addRange(l: any, h: any): void;
        addInterval(toAdd: any): void;
        addSet(other: any): any;
        reduce(pos: any): void;
        complement(start: any, stop: any): any;
        contains(item: any): boolean;
        removeRange(toRemove: any): void;
        removeOne(value: any): void;
        toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
        toCharString(): string;
        toIndexString(): any;
        toTokenString(literalNames: any, symbolicNames: any): any;
        elementName(literalNames: any, symbolicNames: any, token: any): any;
        readonly length: any;
    };
    /**
     * Compute the set of valid tokens that can occur starting in {@code s} and
     * staying in same rule. {@link Token//EPSILON} is in set if we reach end of
     * rule
     */
    nextTokensNoContext(s: any): any;
    nextTokens(s: any, ctx: any): any;
    addState(state: any): void;
    removeState(state: any): void;
    defineDecisionState(s: any): any;
    getDecisionState(decision: any): any;
    /**
     * Computes the set of input symbols which could follow ATN state number
     * {@code stateNumber} in the specified full {@code context}. This method
     * considers the complete parser context, but does not evaluate semantic
     * predicates (i.e. all predicates encountered during the calculation are
     * assumed true). If a path in the ATN exists from the starting state to the
     * {@link RuleStopState} of the outermost context without matching any
     * symbols, {@link Token//EOF} is added to the returned set.
     *
     * <p>If {@code context} is {@code null}, it is treated as
     * {@link ParserRuleContext//EMPTY}.</p>
     *
     * @param stateNumber the ATN state number
     * @param ctx the full parse context
     *
     * @return {IntervalSet} The set of potentially valid input symbols which could follow the
     * specified state in the specified context.
     *
     * @throws IllegalArgumentException if the ATN does not contain a state with
     * number {@code stateNumber}
     */
    getExpectedTokens(stateNumber: any, ctx: any): {
        intervals: any[] | null;
        readOnly: boolean;
        first(v: any): any;
        addOne(v: any): void;
        addRange(l: any, h: any): void;
        addInterval(toAdd: any): void;
        addSet(other: any): any;
        reduce(pos: any): void;
        complement(start: any, stop: any): any;
        contains(item: any): boolean;
        removeRange(toRemove: any): void;
        removeOne(value: any): void;
        toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
        toCharString(): string;
        toIndexString(): any;
        toTokenString(literalNames: any, symbolicNames: any): any;
        elementName(literalNames: any, symbolicNames: any, token: any): any;
        readonly length: any;
    };
}
export class ATNDeserializer {
    constructor(options: any);
    deserializationOptions: any;
    stateFactories: ((() => {
        stateType: number;
        atn: any;
        stateNumber: number;
        ruleIndex: number;
        epsilonOnlyTransitions: boolean;
        transitions: any[];
        nextTokenWithinRule: any;
        toString(): number;
        equals(other: any): boolean;
        isNonGreedyExitState(): boolean;
        addTransition(trans: any, index: any): void;
    }) | null)[] | null;
    actionFactories: {}[] | null;
    deserialize(data: any): {
        /**
         * Used for runtime deserialization of ATNs from strings
         * The type of the ATN.
        */
        grammarType: any;
        maxTokenType: any;
        states: any[];
        /**
         * Each subrule/rule is a decision point and we must track them so we
         * can go back later and build DFA predictors for them.  This includes
         * all the rules, subrules, optional blocks, ()+, ()* etc...
         */
        decisionToState: any[];
        ruleToStartState: any[];
        ruleToStopState: any;
        modeNameToStartState: {};
        /**
         * For lexer ATNs, this maps the rule index to the resulting token type.
         * For parser ATNs, this maps the rule index to the generated bypass token
         * type if the {@link ATNDeserializationOptions//isGenerateRuleBypassTransitions}
         * deserialization option was specified; otherwise, this is {@code null}
         */
        ruleToTokenType: any;
        /**
         * For lexer ATNs, this is an array of {@link LexerAction} objects which may
         * be referenced by action transitions in the ATN
         */
        lexerActions: any;
        modeToStartState: any[];
        /**
         * Compute the set of valid tokens that can occur starting in state {@code s}.
         * If {@code ctx} is null, the set of tokens will not include what can follow
         * the rule surrounding {@code s}. In other words, the set will be
         * restricted to tokens reachable staying within {@code s}'s rule
         */
        nextTokensInContext(s: any, ctx: any): {
            intervals: any[] | null;
            readOnly: boolean;
            first(v: any): any;
            addOne(v: any): void;
            addRange(l: any, h: any): void;
            addInterval(toAdd: any): void;
            addSet(other: any): any;
            reduce(pos: any): void;
            complement(start: any, stop: any): any;
            contains(item: any): boolean;
            removeRange(toRemove: any): void;
            removeOne(value: any): void;
            toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
            toCharString(): string;
            toIndexString(): any;
            toTokenString(literalNames: any, symbolicNames: any): any;
            elementName(literalNames: any, symbolicNames: any, token: any): any;
            readonly length: any;
        };
        /**
         * Compute the set of valid tokens that can occur starting in {@code s} and
         * staying in same rule. {@link Token//EPSILON} is in set if we reach end of
         * rule
         */
        nextTokensNoContext(s: any): any;
        nextTokens(s: any, ctx: any): any;
        addState(state: any): void;
        removeState(state: any): void;
        defineDecisionState(s: any): any;
        getDecisionState(decision: any): any;
        /**
         * Computes the set of input symbols which could follow ATN state number
         * {@code stateNumber} in the specified full {@code context}. This method
         * considers the complete parser context, but does not evaluate semantic
         * predicates (i.e. all predicates encountered during the calculation are
         * assumed true). If a path in the ATN exists from the starting state to the
         * {@link RuleStopState} of the outermost context without matching any
         * symbols, {@link Token//EOF} is added to the returned set.
         *
         * <p>If {@code context} is {@code null}, it is treated as
         * {@link ParserRuleContext//EMPTY}.</p>
         *
         * @param stateNumber the ATN state number
         * @param ctx the full parse context
         *
         * @return {IntervalSet} The set of potentially valid input symbols which could follow the
         * specified state in the specified context.
         *
         * @throws IllegalArgumentException if the ATN does not contain a state with
         * number {@code stateNumber}
         */
        getExpectedTokens(stateNumber: any, ctx: any): {
            intervals: any[] | null;
            readOnly: boolean;
            first(v: any): any;
            addOne(v: any): void;
            addRange(l: any, h: any): void;
            addInterval(toAdd: any): void;
            addSet(other: any): any;
            reduce(pos: any): void;
            complement(start: any, stop: any): any;
            contains(item: any): boolean;
            removeRange(toRemove: any): void;
            removeOne(value: any): void;
            toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
            toCharString(): string;
            toIndexString(): any;
            toTokenString(literalNames: any, symbolicNames: any): any;
            elementName(literalNames: any, symbolicNames: any, token: any): any;
            readonly length: any;
        };
    };
    reset(data: any): boolean;
    data: any;
    pos: number | undefined;
    skipUUID(): void;
    checkVersion(legacy: any): void;
    readATN(): {
        /**
         * Used for runtime deserialization of ATNs from strings
         * The type of the ATN.
        */
        grammarType: any;
        maxTokenType: any;
        states: any[];
        /**
         * Each subrule/rule is a decision point and we must track them so we
         * can go back later and build DFA predictors for them.  This includes
         * all the rules, subrules, optional blocks, ()+, ()* etc...
         */
        decisionToState: any[];
        ruleToStartState: any[];
        ruleToStopState: any;
        modeNameToStartState: {};
        /**
         * For lexer ATNs, this maps the rule index to the resulting token type.
         * For parser ATNs, this maps the rule index to the generated bypass token
         * type if the {@link ATNDeserializationOptions//isGenerateRuleBypassTransitions}
         * deserialization option was specified; otherwise, this is {@code null}
         */
        ruleToTokenType: any;
        /**
         * For lexer ATNs, this is an array of {@link LexerAction} objects which may
         * be referenced by action transitions in the ATN
         */
        lexerActions: any;
        modeToStartState: any[];
        /**
         * Compute the set of valid tokens that can occur starting in state {@code s}.
         * If {@code ctx} is null, the set of tokens will not include what can follow
         * the rule surrounding {@code s}. In other words, the set will be
         * restricted to tokens reachable staying within {@code s}'s rule
         */
        nextTokensInContext(s: any, ctx: any): {
            intervals: any[] | null;
            readOnly: boolean;
            first(v: any): any;
            addOne(v: any): void;
            addRange(l: any, h: any): void;
            addInterval(toAdd: any): void;
            addSet(other: any): any;
            reduce(pos: any): void;
            complement(start: any, stop: any): any;
            contains(item: any): boolean;
            removeRange(toRemove: any): void;
            removeOne(value: any): void;
            toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
            toCharString(): string;
            toIndexString(): any;
            toTokenString(literalNames: any, symbolicNames: any): any;
            elementName(literalNames: any, symbolicNames: any, token: any): any;
            readonly length: any;
        };
        /**
         * Compute the set of valid tokens that can occur starting in {@code s} and
         * staying in same rule. {@link Token//EPSILON} is in set if we reach end of
         * rule
         */
        nextTokensNoContext(s: any): any;
        nextTokens(s: any, ctx: any): any;
        addState(state: any): void;
        removeState(state: any): void;
        defineDecisionState(s: any): any;
        getDecisionState(decision: any): any;
        /**
         * Computes the set of input symbols which could follow ATN state number
         * {@code stateNumber} in the specified full {@code context}. This method
         * considers the complete parser context, but does not evaluate semantic
         * predicates (i.e. all predicates encountered during the calculation are
         * assumed true). If a path in the ATN exists from the starting state to the
         * {@link RuleStopState} of the outermost context without matching any
         * symbols, {@link Token//EOF} is added to the returned set.
         *
         * <p>If {@code context} is {@code null}, it is treated as
         * {@link ParserRuleContext//EMPTY}.</p>
         *
         * @param stateNumber the ATN state number
         * @param ctx the full parse context
         *
         * @return {IntervalSet} The set of potentially valid input symbols which could follow the
         * specified state in the specified context.
         *
         * @throws IllegalArgumentException if the ATN does not contain a state with
         * number {@code stateNumber}
         */
        getExpectedTokens(stateNumber: any, ctx: any): {
            intervals: any[] | null;
            readOnly: boolean;
            first(v: any): any;
            addOne(v: any): void;
            addRange(l: any, h: any): void;
            addInterval(toAdd: any): void;
            addSet(other: any): any;
            reduce(pos: any): void;
            complement(start: any, stop: any): any;
            contains(item: any): boolean;
            removeRange(toRemove: any): void;
            removeOne(value: any): void;
            toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
            toCharString(): string;
            toIndexString(): any;
            toTokenString(literalNames: any, symbolicNames: any): any;
            elementName(literalNames: any, symbolicNames: any, token: any): any;
            readonly length: any;
        };
    };
    readStates(atn: any, legacy: any): void;
    readRules(atn: any, legacy: any): void;
    readModes(atn: any): void;
    readSets(atn: any, sets: any, reader: any): void;
    readEdges(atn: any, sets: any): void;
    readDecisions(atn: any): void;
    readLexerActions(atn: any, legacy: any): void;
    generateRuleBypassTransitions(atn: any): void;
    generateRuleBypassTransition(atn: any, idx: any): void;
    stateIsEndStateFor(state: any, idx: any): {
        stateType: number;
        loopBackState: any;
        isPrecedenceDecision: any;
        decision: number;
        nonGreedy: boolean;
        atn: any;
        stateNumber: number;
        ruleIndex: number;
        epsilonOnlyTransitions: boolean;
        transitions: any[];
        nextTokenWithinRule: any;
        toString(): number;
        equals(other: any): boolean;
        isNonGreedyExitState(): boolean;
        addTransition(trans: any, index: any): void;
    } | null;
    /**
     * Analyze the {@link StarLoopEntryState} states in the specified ATN to set
     * the {@link StarLoopEntryState//isPrecedenceDecision} field to the
     * correct value.
     * @param atn The ATN.
     */
    markPrecedenceDecisions(atn: any): void;
    verifyATN(atn: any): void;
    checkCondition(condition: any, message: any): void;
    readInt(): any;
    readInt32(): number;
    edgeFactory(atn: any, type: any, src: any, trg: any, arg1: any, arg2: any, arg3: any, sets: any): {
        ruleIndex: any;
        precedence: any;
        followState: any;
        serializationType: number;
        isEpsilon: boolean;
        matches(symbol: any, minVocabSymbol: any, maxVocabSymbol: any): boolean;
        target: any;
        label: any;
    } | {
        serializationType: number;
        label: any;
        matches(symbol: any, minVocabSymbol: any, maxVocabSymbol: any): any;
        toString(): any;
        target: any;
        isEpsilon: boolean;
    } | {
        serializationType: number;
        matches(symbol: any, minVocabSymbol: any, maxVocabSymbol: any): boolean;
        toString(): string;
        target: any;
        isEpsilon: boolean;
        label: any;
    } | {
        label_: any;
        label: {
            intervals: any[] | null;
            readOnly: boolean;
            first(v: any): any;
            addOne(v: any): void;
            addRange(l: any, h: any): void;
            addInterval(toAdd: any): void;
            addSet(other: any): any;
            reduce(pos: any): void;
            complement(start: any, stop: any): any;
            contains(item: any): boolean;
            removeRange(toRemove: any): void;
            removeOne(value: any): void;
            toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
            toCharString(): string;
            toIndexString(): any;
            toTokenString(literalNames: any, symbolicNames: any): any;
            elementName(literalNames: any, symbolicNames: any, token: any): any;
            readonly length: any;
        };
        serializationType: number;
        makeLabel(): {
            intervals: any[] | null;
            readOnly: boolean;
            first(v: any): any;
            addOne(v: any): void;
            addRange(l: any, h: any): void;
            addInterval(toAdd: any): void;
            addSet(other: any): any;
            reduce(pos: any): void;
            complement(start: any, stop: any): any;
            contains(item: any): boolean;
            removeRange(toRemove: any): void;
            removeOne(value: any): void;
            toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
            toCharString(): string;
            toIndexString(): any;
            toTokenString(literalNames: any, symbolicNames: any): any;
            elementName(literalNames: any, symbolicNames: any, token: any): any;
            readonly length: any;
        };
        matches(symbol: any, minVocabSymbol: any, maxVocabSymbol: any): boolean;
        toString(): any;
        target: any;
        isEpsilon: boolean;
    } | {
        serializationType: number;
        start: any;
        stop: any;
        label: {
            intervals: any[] | null;
            readOnly: boolean;
            first(v: any): any;
            addOne(v: any): void;
            addRange(l: any, h: any): void;
            addInterval(toAdd: any): void;
            addSet(other: any): any;
            reduce(pos: any): void;
            complement(start: any, stop: any): any;
            contains(item: any): boolean;
            removeRange(toRemove: any): void;
            removeOne(value: any): void;
            toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
            toCharString(): string;
            toIndexString(): any;
            toTokenString(literalNames: any, symbolicNames: any): any;
            elementName(literalNames: any, symbolicNames: any, token: any): any;
            readonly length: any;
        };
        makeLabel(): {
            intervals: any[] | null;
            readOnly: boolean;
            first(v: any): any;
            addOne(v: any): void;
            addRange(l: any, h: any): void;
            addInterval(toAdd: any): void;
            addSet(other: any): any;
            reduce(pos: any): void;
            complement(start: any, stop: any): any;
            contains(item: any): boolean;
            removeRange(toRemove: any): void;
            removeOne(value: any): void;
            toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
            toCharString(): string;
            toIndexString(): any;
            toTokenString(literalNames: any, symbolicNames: any): any;
            elementName(literalNames: any, symbolicNames: any, token: any): any;
            readonly length: any;
        };
        matches(symbol: any, minVocabSymbol: any, maxVocabSymbol: any): boolean;
        toString(): string;
        target: any;
        isEpsilon: boolean;
    } | {
        serializationType: number;
        ruleIndex: any;
        actionIndex: any;
        isCtxDependent: any;
        isEpsilon: boolean;
        matches(symbol: any, minVocabSymbol: any, maxVocabSymbol: any): boolean;
        toString(): string;
        target: any;
        label: any;
    } | {
        serializationType: number;
        isEpsilon: boolean;
        outermostPrecedenceReturn: any;
        matches(symbol: any, minVocabSymbol: any, maxVocabSymbol: any): boolean;
        toString(): string;
        target: any;
        label: any;
    } | {
        serializationType: number;
        ruleIndex: any;
        predIndex: any;
        isCtxDependent: any;
        isEpsilon: boolean;
        matches(symbol: any, minVocabSymbol: any, maxVocabSymbol: any): boolean;
        getPredicate(): {
            ruleIndex: any;
            predIndex: any;
            isCtxDependent: any;
            evaluate(parser: any, outerContext: any): any;
            updateHashCode(hash: any): void;
            equals(other: any): boolean;
            toString(): string;
            hashCode(): number;
            /**
             * Evaluate the precedence predicates for the context and reduce the result.
             *
             * @param parser The parser instance.
             * @param outerContext The current parser context object.
             * @return The simplified semantic context after precedence predicates are
             * evaluated, which will be one of the following values.
             * <ul>
             * <li>{@link //NONE}: if the predicate simplifies to {@code true} after
             * precedence predicates are evaluated.</li>
             * <li>{@code null}: if the predicate simplifies to {@code false} after
             * precedence predicates are evaluated.</li>
             * <li>{@code this}: if the semantic context is not changed as a result of
             * precedence predicate evaluation.</li>
             * <li>A non-{@code null} {@link SemanticContext}: the new simplified
             * semantic context after precedence predicates are evaluated.</li>
             * </ul>
             */
            evalPrecedence(parser: any, outerContext: any): any;
        };
        toString(): string;
        target: any;
        label: any;
    } | {
        serializationType: number;
        precedence: any;
        isEpsilon: boolean;
        matches(symbol: any, minVocabSymbol: any, maxVocabSymbol: any): boolean;
        getPredicate(): {
            precedence: any;
            evaluate(parser: any, outerContext: any): any;
            evalPrecedence(parser: any, outerContext: any): {
                ruleIndex: any;
                predIndex: any;
                isCtxDependent: any;
                evaluate(parser: any, outerContext: any): any;
                updateHashCode(hash: any): void;
                equals(other: any): boolean;
                toString(): string;
                hashCode(): number;
                /**
                 * Evaluate the precedence predicates for the context and reduce the result.
                 *
                 * @param parser The parser instance.
                 * @param outerContext The current parser context object.
                 * @return The simplified semantic context after precedence predicates are
                 * evaluated, which will be one of the following values.
                 * <ul>
                 * <li>{@link //NONE}: if the predicate simplifies to {@code true} after
                 * precedence predicates are evaluated.</li>
                 * <li>{@code null}: if the predicate simplifies to {@code false} after
                 * precedence predicates are evaluated.</li>
                 * <li>{@code this}: if the semantic context is not changed as a result of
                 * precedence predicate evaluation.</li>
                 * <li>A non-{@code null} {@link SemanticContext}: the new simplified
                 * semantic context after precedence predicates are evaluated.</li>
                 * </ul>
                 */
                evalPrecedence(parser: any, outerContext: any): any;
            } | null;
            compareTo(other: any): number;
            updateHashCode(hash: any): void;
            equals(other: any): boolean;
            toString(): string;
            hashCode(): number;
        };
        toString(): string;
        target: any;
        label: any;
    };
    stateFactory(type: any, ruleIndex: any): {
        stateType: number;
        atn: any;
        stateNumber: number;
        ruleIndex: number;
        epsilonOnlyTransitions: boolean;
        transitions: any[];
        nextTokenWithinRule: any;
        toString(): number;
        equals(other: any): boolean;
        isNonGreedyExitState(): boolean;
        addTransition(trans: any, index: any): void;
    } | undefined;
    lexerActionFactory(type: any, data1: any, data2: any): any;
}
declare const BailErrorStrategy_base: {
    new (): {
        /**
         * Indicates whether the error strategy is currently "recovering from an
         * error". This is used to suppress reporting multiple error messages while
         * attempting to recover from a detected syntax error.
         *
         * @see //inErrorRecoveryMode
         */
        errorRecoveryMode: boolean;
        /**
         * The index into the input stream where the last error occurred.
         * This is used to prevent infinite loops where an error is found
         * but no token is consumed during recovery...another error is found,
         * ad nauseum. This is a failsafe mechanism to guarantee that at least
         * one token/tree node is consumed for two errors.
         */
        lastErrorIndex: number;
        lastErrorStates: any[] | null;
        nextTokensContext: any;
        nextTokenState: number;
        /**
         * <p>The default implementation simply calls {@link //endErrorCondition} to
         * ensure that the handler is not in error recovery mode.</p>
         */
        reset(recognizer: any): void;
        /**
         * This method is called to enter error recovery mode when a recognition
         * exception is reported.
         *
         * @param recognizer the parser instance
         */
        beginErrorCondition(recognizer: any): void;
        inErrorRecoveryMode(recognizer: any): boolean;
        /**
         * This method is called to leave error recovery mode after recovering from
         * a recognition exception.
         * @param recognizer
         */
        endErrorCondition(recognizer: any): void;
        /**
         * {@inheritDoc}
         * <p>The default implementation simply calls {@link //endErrorCondition}.</p>
         */
        reportMatch(recognizer: any): void;
        /**
         * {@inheritDoc}
         *
         * <p>The default implementation returns immediately if the handler is already
         * in error recovery mode. Otherwise, it calls {@link //beginErrorCondition}
         * and dispatches the reporting task based on the runtime type of {@code e}
         * according to the following table.</p>
         *
         * <ul>
         * <li>{@link NoViableAltException}: Dispatches the call to
         * {@link //reportNoViableAlternative}</li>
         * <li>{@link InputMismatchException}: Dispatches the call to
         * {@link //reportInputMismatch}</li>
         * <li>{@link FailedPredicateException}: Dispatches the call to
         * {@link //reportFailedPredicate}</li>
         * <li>All other types: calls {@link Parser//notifyErrorListeners} to report
         * the exception</li>
         * </ul>
         */
        reportError(recognizer: any, e: any): void;
        /**
         *
         * {@inheritDoc}
         *
         * <p>The default implementation resynchronizes the parser by consuming tokens
         * until we find one in the resynchronization set--loosely the set of tokens
         * that can follow the current rule.</p>
         *
         */
        recover(recognizer: any, e: any): void;
        /**
         * The default implementation of {@link ANTLRErrorStrategy//sync} makes sure
         * that the current lookahead symbol is consistent with what were expecting
         * at this point in the ATN. You can call this anytime but ANTLR only
         * generates code to check before subrules/loops and each iteration.
         *
         * <p>Implements Jim Idle's magic sync mechanism in closures and optional
         * subrules. E.g.,</p>
         *
         * <pre>
         * a : sync ( stuff sync )* ;
         * sync : {consume to what can follow sync} ;
         * </pre>
         *
         * At the start of a sub rule upon error, {@link //sync} performs single
         * token deletion, if possible. If it can't do that, it bails on the current
         * rule and uses the default error recovery, which consumes until the
         * resynchronization set of the current rule.
         *
         * <p>If the sub rule is optional ({@code (...)?}, {@code (...)*}, or block
         * with an empty alternative), then the expected set includes what follows
         * the subrule.</p>
         *
         * <p>During loop iteration, it consumes until it sees a token that can start a
         * sub rule or what follows loop. Yes, that is pretty aggressive. We opt to
         * stay in the loop as long as possible.</p>
         *
         * <p><strong>ORIGINS</strong></p>
         *
         * <p>Previous versions of ANTLR did a poor job of their recovery within loops.
         * A single mismatch token or missing token would force the parser to bail
         * out of the entire rules surrounding the loop. So, for rule</p>
         *
         * <pre>
         * classDef : 'class' ID '{' member* '}'
         * </pre>
         *
         * input with an extra token between members would force the parser to
         * consume until it found the next class definition rather than the next
         * member definition of the current class.
         *
         * <p>This functionality cost a little bit of effort because the parser has to
         * compare token set at the start of the loop and at each iteration. If for
         * some reason speed is suffering for you, you can turn off this
         * functionality by simply overriding this method as a blank { }.</p>
         *
         */
        sync(recognizer: any): void;
        nextTokensState: any;
        /**
         * This is called by {@link //reportError} when the exception is a
         * {@link NoViableAltException}.
         *
         * @see //reportError
         *
         * @param recognizer the parser instance
         * @param e the recognition exception
         */
        reportNoViableAlternative(recognizer: any, e: any): void;
        /**
         * This is called by {@link //reportError} when the exception is an
         * {@link InputMismatchException}.
         *
         * @see //reportError
         *
         * @param recognizer the parser instance
         * @param e the recognition exception
         */
        reportInputMismatch(recognizer: any, e: any): void;
        /**
         * This is called by {@link //reportError} when the exception is a
         * {@link FailedPredicateException}.
         *
         * @see //reportError
         *
         * @param recognizer the parser instance
         * @param e the recognition exception
         */
        reportFailedPredicate(recognizer: any, e: any): void;
        /**
         * This method is called to report a syntax error which requires the removal
         * of a token from the input stream. At the time this method is called, the
         * erroneous symbol is current {@code LT(1)} symbol and has not yet been
         * removed from the input stream. When this method returns,
         * {@code recognizer} is in error recovery mode.
         *
         * <p>This method is called when {@link //singleTokenDeletion} identifies
         * single-token deletion as a viable recovery strategy for a mismatched
         * input error.</p>
         *
         * <p>The default implementation simply returns if the handler is already in
         * error recovery mode. Otherwise, it calls {@link //beginErrorCondition} to
         * enter error recovery mode, followed by calling
         * {@link Parser//notifyErrorListeners}.</p>
         *
         * @param recognizer the parser instance
         *
         */
        reportUnwantedToken(recognizer: any): void;
        /**
         * This method is called to report a syntax error which requires the
         * insertion of a missing token into the input stream. At the time this
         * method is called, the missing token has not yet been inserted. When this
         * method returns, {@code recognizer} is in error recovery mode.
         *
         * <p>This method is called when {@link //singleTokenInsertion} identifies
         * single-token insertion as a viable recovery strategy for a mismatched
         * input error.</p>
         *
         * <p>The default implementation simply returns if the handler is already in
         * error recovery mode. Otherwise, it calls {@link //beginErrorCondition} to
         * enter error recovery mode, followed by calling
         * {@link Parser//notifyErrorListeners}.</p>
         *
         * @param recognizer the parser instance
         */
        reportMissingToken(recognizer: any): void;
        /**
         * <p>The default implementation attempts to recover from the mismatched input
         * by using single token insertion and deletion as described below. If the
         * recovery attempt fails, this method throws an
         * {@link InputMismatchException}.</p>
         *
         * <p><strong>EXTRA TOKEN</strong> (single token deletion)</p>
         *
         * <p>{@code LA(1)} is not what we are looking for. If {@code LA(2)} has the
         * right token, however, then assume {@code LA(1)} is some extra spurious
         * token and delete it. Then consume and return the next token (which was
         * the {@code LA(2)} token) as the successful result of the match operation.</p>
         *
         * <p>This recovery strategy is implemented by {@link
            * //singleTokenDeletion}.</p>
         *
         * <p><strong>MISSING TOKEN</strong> (single token insertion)</p>
         *
         * <p>If current token (at {@code LA(1)}) is consistent with what could come
         * after the expected {@code LA(1)} token, then assume the token is missing
         * and use the parser's {@link TokenFactory} to create it on the fly. The
         * "insertion" is performed by returning the created token as the successful
         * result of the match operation.</p>
         *
         * <p>This recovery strategy is implemented by {@link
            * //singleTokenInsertion}.</p>
         *
         * <p><strong>EXAMPLE</strong></p>
         *
         * <p>For example, Input {@code i=(3;} is clearly missing the {@code ')'}. When
         * the parser returns from the nested call to {@code expr}, it will have
         * call chain:</p>
         *
         * <pre>
         * stat &rarr; expr &rarr; atom
         * </pre>
         *
         * and it will be trying to match the {@code ')'} at this point in the
         * derivation:
         *
         * <pre>
         * =&gt; ID '=' '(' INT ')' ('+' atom)* ';'
         * ^
         * </pre>
         *
         * The attempt to match {@code ')'} will fail when it sees {@code ';'} and
         * call {@link //recoverInline}. To recover, it sees that {@code LA(1)==';'}
         * is in the set of tokens that can follow the {@code ')'} token reference
         * in rule {@code atom}. It can assume that you forgot the {@code ')'}.
         */
        recoverInline(recognizer: any): any;
        /**
         * This method implements the single-token insertion inline error recovery
         * strategy. It is called by {@link //recoverInline} if the single-token
         * deletion strategy fails to recover from the mismatched input. If this
         * method returns {@code true}, {@code recognizer} will be in error recovery
         * mode.
         *
         * <p>This method determines whether or not single-token insertion is viable by
         * checking if the {@code LA(1)} input symbol could be successfully matched
         * if it were instead the {@code LA(2)} symbol. If this method returns
         * {@code true}, the caller is responsible for creating and inserting a
         * token with the correct type to produce this behavior.</p>
         *
         * @param recognizer the parser instance
         * @return {@code true} if single-token insertion is a viable recovery
         * strategy for the current mismatched input, otherwise {@code false}
         */
        singleTokenInsertion(recognizer: any): any;
        /**
         * This method implements the single-token deletion inline error recovery
         * strategy. It is called by {@link //recoverInline} to attempt to recover
         * from mismatched input. If this method returns null, the parser and error
         * handler state will not have changed. If this method returns non-null,
         * {@code recognizer} will <em>not</em> be in error recovery mode since the
         * returned token was a successful match.
         *
         * <p>If the single-token deletion is successful, this method calls
         * {@link //reportUnwantedToken} to report the error, followed by
         * {@link Parser//consume} to actually "delete" the extraneous token. Then,
         * before returning {@link //reportMatch} is called to signal a successful
         * match.</p>
         *
         * @param recognizer the parser instance
         * @return the successfully matched {@link Token} instance if single-token
         * deletion successfully recovers from the mismatched input, otherwise
         * {@code null}
         */
        singleTokenDeletion(recognizer: any): any;
        /**
         * Conjure up a missing token during error recovery.
         *
         * The recognizer attempts to recover from single missing
         * symbols. But, actions might refer to that missing symbol.
         * For example, x=ID {f($x);}. The action clearly assumes
         * that there has been an identifier matched previously and that
         * $x points at that token. If that token is missing, but
         * the next token in the stream is what we want we assume that
         * this token is missing and we keep going. Because we
         * have to return some token to replace the missing token,
         * we have to conjure one up. This method gives the user control
         * over the tokens returned for missing tokens. Mostly,
         * you will want to create something special for identifier
         * tokens. For literals such as '{' and ',', the default
         * action in the parser or tree parser works. It simply creates
         * a CommonToken of the appropriate type. The text will be the token.
         * If you change what tokens must be created by the lexer,
         * override this method to create the appropriate tokens.
         *
         */
        getMissingSymbol(recognizer: any): any;
        getExpectedTokens(recognizer: any): any;
        /**
         * How should a token be displayed in an error message? The default
         * is to display just the text, but during development you might
         * want to have a lot of information spit out. Override in that case
         * to use t.toString() (which, for CommonToken, dumps everything about
         * the token). This is better than forcing you to override a method in
         * your token objects because you don't have to go modify your lexer
         * so that it creates a new Java type.
         */
        getTokenErrorDisplay(t: any): string;
        escapeWSAndQuote(s: any): string;
        /**
         * Compute the error recovery set for the current rule. During
         * rule invocation, the parser pushes the set of tokens that can
         * follow that rule reference on the stack; this amounts to
         * computing FIRST of what follows the rule reference in the
         * enclosing rule. See LinearApproximator.FIRST().
         * This local follow set only includes tokens
         * from within the rule; i.e., the FIRST computation done by
         * ANTLR stops at the end of a rule.
         *
         * EXAMPLE
         *
         * When you find a "no viable alt exception", the input is not
         * consistent with any of the alternatives for rule r. The best
         * thing to do is to consume tokens until you see something that
         * can legally follow a call to r//or* any rule that called r.
         * You don't want the exact set of viable next tokens because the
         * input might just be missing a token--you might consume the
         * rest of the input looking for one of the missing tokens.
         *
         * Consider grammar:
         *
         * a : '[' b ']'
         * | '(' b ')'
         * ;
         * b : c '^' INT ;
         * c : ID
         * | INT
         * ;
         *
         * At each rule invocation, the set of tokens that could follow
         * that rule is pushed on a stack. Here are the various
         * context-sensitive follow sets:
         *
         * FOLLOW(b1_in_a) = FIRST(']') = ']'
         * FOLLOW(b2_in_a) = FIRST(')') = ')'
         * FOLLOW(c_in_b) = FIRST('^') = '^'
         *
         * Upon erroneous input "[]", the call chain is
         *
         * a -> b -> c
         *
         * and, hence, the follow context stack is:
         *
         * depth follow set start of rule execution
         * 0 <EOF> a (from main())
         * 1 ']' b
         * 2 '^' c
         *
         * Notice that ')' is not included, because b would have to have
         * been called from a different context in rule a for ')' to be
         * included.
         *
         * For error recovery, we cannot consider FOLLOW(c)
         * (context-sensitive or otherwise). We need the combined set of
         * all context-sensitive FOLLOW sets--the set of all tokens that
         * could follow any reference in the call chain. We need to
         * resync to one of those tokens. Note that FOLLOW(c)='^' and if
         * we resync'd to that token, we'd consume until EOF. We need to
         * sync to context-sensitive FOLLOWs for a, b, and c: {']','^'}.
         * In this case, for input "[]", LA(1) is ']' and in the set, so we would
         * not consume anything. After printing an error, rule c would
         * return normally. Rule b would not find the required '^' though.
         * At this point, it gets a mismatched token error and throws an
         * exception (since LA(1) is not in the viable following token
         * set). The rule exception handler tries to recover, but finds
         * the same recovery set and doesn't consume anything. Rule b
         * exits normally returning to rule a. Now it finds the ']' (and
         * with the successful match exits errorRecovery mode).
         *
         * So, you can see that the parser walks up the call chain looking
         * for the token that was a member of the recovery set.
         *
         * Errors are not generated in errorRecovery mode.
         *
         * ANTLR's error recovery mechanism is based upon original ideas:
         *
         * "Algorithms + Data Structures = Programs" by Niklaus Wirth
         *
         * and
         *
         * "A note on error recovery in recursive descent parsers":
         * http://portal.acm.org/citation.cfm?id=947902.947905
         *
         * Later, Josef Grosch had some good ideas:
         *
         * "Efficient and Comfortable Error Recovery in Recursive Descent
         * Parsers":
         * ftp://www.cocolab.com/products/cocktail/doca4.ps/ell.ps.zip
         *
         * Like Grosch I implement context-sensitive FOLLOW sets that are combined
         * at run-time upon error to avoid overhead during parsing.
         */
        getErrorRecoverySet(recognizer: any): {
            intervals: any[] | null;
            readOnly: boolean;
            first(v: any): any;
            addOne(v: any): void;
            addRange(l: any, h: any): void;
            addInterval(toAdd: any): void;
            addSet(other: any): any;
            reduce(pos: any): void;
            complement(start: any, stop: any): any;
            contains(item: any): boolean;
            removeRange(toRemove: any): void;
            removeOne(value: any): void;
            toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
            toCharString(): string;
            toIndexString(): any;
            toTokenString(literalNames: any, symbolicNames: any): any;
            elementName(literalNames: any, symbolicNames: any, token: any): any;
            readonly length: any;
        };
        consumeUntil(recognizer: any, set: any): void;
    };
};
/**
 * This implementation of {@link ANTLRErrorStrategy} responds to syntax errors
 * by immediately canceling the parse operation with a
 * {@link ParseCancellationException}. The implementation ensures that the
 * {@link ParserRuleContext//exception} field is set for all parse tree nodes
 * that were not completed prior to encountering the error.
 *
 * <p>
 * This error strategy is useful in the following scenarios.</p>
 *
 * <ul>
 * <li><strong>Two-stage parsing:</strong> This error strategy allows the first
 * stage of two-stage parsing to immediately terminate if an error is
 * encountered, and immediately fall back to the second stage. In addition to
 * avoiding wasted work by attempting to recover from errors here, the empty
 * implementation of {@link BailErrorStrategy//sync} improves the performance of
 * the first stage.</li>
 * <li><strong>Silent validation:</strong> When syntax errors are not being
 * reported or logged, and the parse result is simply ignored if errors occur,
 * the {@link BailErrorStrategy} avoids wasting work on recovering from errors
 * when the result will be ignored either way.</li>
 * </ul>
 *
 * <p>
 * {@code myparser.setErrorHandler(new BailErrorStrategy());}</p>
 *
 * @see Parser//setErrorHandler(ANTLRErrorStrategy)
 * */
export class BailErrorStrategy extends BailErrorStrategy_base {
    /**
     * Make sure we don't attempt to recover inline; if the parser
     * successfully recovers, it won't throw an exception.
     */
    recoverInline(recognizer: any): void;
}
declare const InputStream_base: {
    new (data: any, decodeToUnicodeCodePoints: any): {
        name: string;
        strdata: any;
        decodeToUnicodeCodePoints: any;
        _index: number;
        data: any[];
        _size: number;
        /**
         * Reset the stream so that it's in the same state it was
         * when the object was created *except* the data array is not
         * touched.
         */
        reset(): void;
        consume(): void;
        LA(offset: any): any;
        LT(offset: any): any;
        mark(): number;
        release(marker: any): void;
        /**
         * consume() ahead until p==_index; can't just set p=_index as we must
         * update line and column. If we seek backwards, just set p
         */
        seek(_index: any): void;
        getText(start: any, stop: any): any;
        toString(): any;
        readonly index: number;
        readonly size: number;
    };
};
/**
 * @deprecated Use CharStream instead
*/
export class InputStream extends InputStream_base {
}
declare const CommonToken_base: {
    new (): {
        source: any;
        type: any;
        channel: any;
        start: any;
        stop: any;
        tokenIndex: any;
        line: any;
        column: any;
        _text: any;
        getTokenSource(): any;
        getInputStream(): any;
        text: any;
    };
    INVALID_TYPE: number;
    /**
     * During lookahead operations, this "token" signifies we hit rule end ATN state
     * and did not follow it despite needing to.
     */
    EPSILON: number;
    MIN_USER_TOKEN_TYPE: number;
    EOF: number;
    /**
     * All tokens go to the parser (unless skip() is called in that rule)
     * on a particular "channel". The parser tunes to a particular channel
     * so that whitespace etc... can go to the parser on a "hidden" channel.
     */
    DEFAULT_CHANNEL: number;
    /**
     * Anything on different channel than DEFAULT_CHANNEL is not parsed
     * by parser.
     */
    HIDDEN_CHANNEL: number;
};
export class CommonToken extends CommonToken_base {
    constructor(source: any, type: any, channel: any, start: any, stop: any);
    tokenIndex: number;
    /**
     * Constructs a new {@link CommonToken} as a copy of another {@link Token}.
     *
     * <p>
     * If {@code oldToken} is also a {@link CommonToken} instance, the newly
     * constructed token will share a reference to the {@link //text} field and
     * the {@link Pair} stored in {@link //source}. Otherwise, {@link //text} will
     * be assigned the result of calling {@link //getText}, and {@link //source}
     * will be constructed from the result of {@link Token//getTokenSource} and
     * {@link Token//getInputStream}.</p>
     *
     * @param oldToken The token to copy.
     */
    clone(): {
        source: any;
        type: any;
        channel: any;
        start: any;
        stop: any;
        tokenIndex: number;
        line: any;
        column: any;
        clone(): any;
        cloneWithType(type: any): any;
        toString(): string;
        text: any;
        _text: any;
        getTokenSource(): any;
        getInputStream(): any;
    };
    cloneWithType(type: any): {
        source: any;
        type: any;
        channel: any;
        start: any;
        stop: any;
        tokenIndex: number;
        line: any;
        column: any;
        /**
         * Constructs a new {@link CommonToken} as a copy of another {@link Token}.
         *
         * <p>
         * If {@code oldToken} is also a {@link CommonToken} instance, the newly
         * constructed token will share a reference to the {@link //text} field and
         * the {@link Pair} stored in {@link //source}. Otherwise, {@link //text} will
         * be assigned the result of calling {@link //getText}, and {@link //source}
         * will be constructed from the result of {@link Token//getTokenSource} and
         * {@link Token//getInputStream}.</p>
         *
         * @param oldToken The token to copy.
         */
        clone(): any;
        cloneWithType(type: any): any;
        toString(): string;
        text: any;
        _text: any;
        getTokenSource(): any;
        getInputStream(): any;
    };
}
declare const CommonTokenStream_base: {
    new (tokenSource: any): {
        tokenSource: any;
        /**
         * A collection of all tokens fetched from the token source. The list is
         * considered a complete view of the input once {@link //fetchedEOF} is set
         * to {@code true}.
         */
        tokens: any[];
        /**
         * The index into {@link //tokens} of the current token (next token to
         * {@link //consume}). {@link //tokens}{@code [}{@link //p}{@code ]} should
         * be
         * {@link //LT LT(1)}.
         *
         * <p>This field is set to -1 when the stream is first constructed or when
         * {@link //setTokenSource} is called, indicating that the first token has
         * not yet been fetched from the token source. For additional information,
         * see the documentation of {@link IntStream} for a description of
         * Initializing Methods.</p>
         */
        index: number;
        /**
         * Indicates whether the {@link Token//EOF} token has been fetched from
         * {@link //tokenSource} and added to {@link //tokens}. This field improves
         * performance for the following cases:
         *
         * <ul>
         * <li>{@link //consume}: The lookahead check in {@link //consume} to
         * prevent
         * consuming the EOF symbol is optimized by checking the values of
         * {@link //fetchedEOF} and {@link //p} instead of calling {@link
         * //LA}.</li>
         * <li>{@link //fetch}: The check to prevent adding multiple EOF symbols
         * into
         * {@link //tokens} is trivial with this field.</li>
         * <ul>
         */
        fetchedEOF: boolean;
        mark(): number;
        release(marker: any): void;
        reset(): void;
        seek(index: any): void;
        get(index: any): any;
        consume(): void;
        /**
         * Make sure index {@code i} in tokens has a token.
         *
         * @return {Boolean} {@code true} if a token is located at index {@code i}, otherwise
         * {@code false}.
         * @see //get(int i)
         */
        sync(i: any): boolean;
        /**
         * Add {@code n} elements to buffer.
         *
         * @return {Number} The actual number of elements added to the buffer.
         */
        fetch(n: any): number;
        getTokens(start: any, stop: any, types: any): any[] | null;
        LA(i: any): any;
        LB(k: any): any;
        LT(k: any): any;
        /**
         * Allowed derived classes to modify the behavior of operations which change
         * the current stream position by adjusting the target token index of a seek
         * operation. The default implementation simply returns {@code i}. If an
         * exception is thrown in this method, the current stream index should not be
         * changed.
         *
         * <p>For example, {@link CommonTokenStream} overrides this method to ensure
         * that
         * the seek target is always an on-channel token.</p>
         *
         * @param {Number} i The target token index.
         * @return {Number} The adjusted target token index.
         */
        adjustSeekIndex(i: number): number;
        lazyInit(): void;
        setup(): void;
        setTokenSource(tokenSource: any): void;
        /**
         * Given a starting index, return the index of the next token on channel.
         * Return i if tokens[i] is on channel. Return -1 if there are no tokens
         * on channel between i and EOF.
         */
        nextTokenOnChannel(i: any, channel: any): any;
        /**
         * Given a starting index, return the index of the previous token on channel.
         * Return i if tokens[i] is on channel. Return -1 if there are no tokens
         * on channel between i and 0.
         */
        previousTokenOnChannel(i: any, channel: any): any;
        /**
         * Collect all tokens on specified channel to the right of
         * the current token up until we see a token on DEFAULT_TOKEN_CHANNEL or
         * EOF. If channel is -1, find any non default channel token.
         */
        getHiddenTokensToRight(tokenIndex: any, channel: any): any[] | null;
        /**
         * Collect all tokens on specified channel to the left of
         * the current token up until we see a token on DEFAULT_TOKEN_CHANNEL.
         * If channel is -1, find any non default channel token.
         */
        getHiddenTokensToLeft(tokenIndex: any, channel: any): any[] | null;
        filterForChannel(left: any, right: any, channel: any): any[] | null;
        getSourceName(): any;
        getText(interval: any): string;
        fill(): void;
    };
    readonly size: any;
};
/**
 * This class extends {@link BufferedTokenStream} with functionality to filter
 * token streams to tokens on a particular channel (tokens where
 * {@link Token//getChannel} returns a particular value).
 *
 * <p>
 * This token stream provides access to all tokens by index or when calling
 * methods like {@link //getText}. The channel filtering is only used for code
 * accessing tokens via the lookahead methods {@link //LA}, {@link //LT}, and
 * {@link //LB}.</p>
 *
 * <p>
 * By default, tokens are placed on the default channel
 * ({@link Token//DEFAULT_CHANNEL}), but may be reassigned by using the
 * {@code ->channel(HIDDEN)} lexer command, or by using an embedded action to
 * call {@link Lexer//setChannel}.
 * </p>
 *
 * <p>
 * Note: lexer rules which use the {@code ->skip} lexer command or call
 * {@link Lexer//skip} do not produce tokens at all, so input text matched by
 * such a rule will not be available as part of the token stream, regardless of
 * channel.</p>
 */
export class CommonTokenStream extends CommonTokenStream_base {
    constructor(lexer: any, channel: any);
    channel: any;
    adjustSeekIndex(i: any): any;
    getNumberOfOnChannelTokens(): number;
}
export class DFA {
    constructor(atnStartState: any, decision: any);
    /**
     * From which ATN state did we create this DFA?
     */
    atnStartState: any;
    decision: any;
    /**
     * A set of all DFA states. Use {@link Map} so we can get old state back
     * ({@link Set} only allows you to see if it's there).
     */
    _states: {
        data: {};
        hashFunction: any;
        equalsFunction: any;
        add(value: any): any;
        has(value: any): boolean;
        get(value: any): any;
        values(): any[];
        toString(): string;
        readonly length: any;
    };
    s0: {
        stateNumber: any;
        configs: any;
        /**
         * {@code edges[symbol]} points to target of symbol. Shift up by 1 so (-1)
         * {@link Token//EOF} maps to {@code edges[0]}.
         */
        edges: any;
        isAcceptState: boolean;
        /**
         * if accept state, what ttype do we match or alt do we predict?
         * This is set to {@link ATN//INVALID_ALT_NUMBER} when {@link//predicates}
         * {@code !=null} or {@link //requiresFullContext}.
         */
        prediction: number;
        lexerActionExecutor: any;
        /**
         * Indicates that this state was created during SLL prediction that
         * discovered a conflict between the configurations in the state. Future
         * {@link ParserATNSimulator//execATN} invocations immediately jumped doing
         * full context prediction if this field is true.
         */
        requiresFullContext: boolean;
        /**
         * During SLL parsing, this is a list of predicates associated with the
         * ATN configurations of the DFA state. When we have predicates,
         * {@link //requiresFullContext} is {@code false} since full context
         * prediction evaluates predicates
         * on-the-fly. If this is not null, then {@link //prediction} is
         * {@link ATN//INVALID_ALT_NUMBER}.
         *
         * <p>We only use these for non-{@link //requiresFullContext} but
         * conflicting states. That
         * means we know from the context (it's $ or we don't dip into outer
         * context) that it's an ambiguity not a conflict.</p>
         *
         * <p>This list is computed by {@link
         * ParserATNSimulator//predicateDFAState}.</p>
         */
        predicates: any;
        /**
         * Get the set of all alts mentioned by all ATN configurations in this
         * DFA state.
         */
        getAltSet(): {
            data: {};
            hashFunction: any;
            equalsFunction: any;
            add(value: any): any;
            has(value: any): boolean;
            get(value: any): any;
            values(): any[];
            toString(): string;
            readonly length: any;
        } | null;
        /**
         * Two {@link DFAState} instances are equal if their ATN configuration sets
         * are the same. This method is used to see if a state already exists.
         *
         * <p>Because the number of alternatives and number of ATN configurations are
         * finite, there is a finite number of DFA states that can be processed.
         * This is necessary to show that the algorithm terminates.</p>
         *
         * <p>Cannot test the DFA state numbers here because in
         * {@link ParserATNSimulator//addDFAState} we need to know if any other state
         * exists that has this exact set of ATN configurations. The
         * {@link //stateNumber} is irrelevant.</p>
         */
        equals(other: any): any;
        toString(): string;
        hashCode(): number;
    } | null;
    /**
     * {@code true} if this DFA is for a precedence decision; otherwise,
     * {@code false}. This is the backing field for {@link //isPrecedenceDfa},
     * {@link //setPrecedenceDfa}
     */
    precedenceDfa: boolean;
    /**
     * Get the start state for a specific precedence value.
     *
     * @param precedence The current precedence.
     * @return The start state corresponding to the specified precedence, or
     * {@code null} if no start state exists for the specified precedence.
     *
     * @throws IllegalStateException if this is not a precedence DFA.
     * @see //isPrecedenceDfa()
     */
    getPrecedenceStartState(precedence: any): any;
    /**
     * Set the start state for a specific precedence value.
     *
     * @param precedence The current precedence.
     * @param startState The start state corresponding to the specified
     * precedence.
     *
     * @throws IllegalStateException if this is not a precedence DFA.
     * @see //isPrecedenceDfa()
     */
    setPrecedenceStartState(precedence: any, startState: any): void;
    /**
     * Sets whether this is a precedence DFA. If the specified value differs
     * from the current DFA configuration, the following actions are taken;
     * otherwise no changes are made to the current DFA.
     *
     * <ul>
     * <li>The {@link //states} map is cleared</li>
     * <li>If {@code precedenceDfa} is {@code false}, the initial state
     * {@link //s0} is set to {@code null}; otherwise, it is initialized to a new
     * {@link DFAState} with an empty outgoing {@link DFAState//edges} array to
     * store the start states for individual precedence values.</li>
     * <li>The {@link //precedenceDfa} field is updated</li>
     * </ul>
     *
     * @param precedenceDfa {@code true} if this is a precedence DFA; otherwise,
     * {@code false}
     */
    setPrecedenceDfa(precedenceDfa: any): void;
    /**
     * Return a list of all states in this DFA, ordered by state number.
     */
    sortedStates(): any[];
    toString(literalNames: any, symbolicNames: any): string | null;
    toLexerString(): string | null;
    get states(): {
        data: {};
        hashFunction: any;
        equalsFunction: any;
        add(value: any): any;
        has(value: any): boolean;
        get(value: any): any;
        values(): any[];
        toString(): string;
        readonly length: any;
    };
}
declare const DiagnosticErrorListener_base: {
    new (): {
        syntaxError(recognizer: any, offendingSymbol: any, line: any, column: any, msg: any, e: any): void;
        reportAmbiguity(recognizer: any, dfa: any, startIndex: any, stopIndex: any, exact: any, ambigAlts: any, configs: any): void;
        reportAttemptingFullContext(recognizer: any, dfa: any, startIndex: any, stopIndex: any, conflictingAlts: any, configs: any): void;
        reportContextSensitivity(recognizer: any, dfa: any, startIndex: any, stopIndex: any, prediction: any, configs: any): void;
    };
};
/**
 * This implementation of {@link ANTLRErrorListener} can be used to identify
 *  certain potential correctness and performance problems in grammars. "Reports"
 *  are made by calling {@link Parser//notifyErrorListeners} with the appropriate
 *  message.
 *
 *  <ul>
 *  <li><b>Ambiguities</b>: These are cases where more than one path through the
 *  grammar can match the input.</li>
 *  <li><b>Weak context sensitivity</b>: These are cases where full-context
 *  prediction resolved an SLL conflict to a unique alternative which equaled the
 *  minimum alternative of the SLL conflict.</li>
 *  <li><b>Strong (forced) context sensitivity</b>: These are cases where the
 *  full-context prediction resolved an SLL conflict to a unique alternative,
 *  <em>and</em> the minimum alternative of the SLL conflict was found to not be
 *  a truly viable alternative. Two-stage parsing cannot be used for inputs where
 *  this situation occurs.</li>
 *  </ul>
 */
export class DiagnosticErrorListener extends DiagnosticErrorListener_base {
    constructor(exactOnly: any);
    exactOnly: any;
    getDecisionDescription(recognizer: any, dfa: any): string;
    /**
     * Computes the set of conflicting or ambiguous alternatives from a
     * configuration set, if that information was not already provided by the
     * parser.
     *
     * @param reportedAlts The set of conflicting or ambiguous alternatives, as
     * reported by the parser.
     * @param configs The conflicting or ambiguous configuration set.
     * @return Returns {@code reportedAlts} if it is not {@code null}, otherwise
     * returns the set of alternatives represented in {@code configs}.
        */
    getConflictingAlts(reportedAlts: any, configs: any): any;
}
/**
 * Provides an empty default implementation of {@link ANTLRErrorListener}. The
 * default implementation of each method does nothing, but can be overridden as
 * necessary.
 */
export class ErrorListener {
    syntaxError(recognizer: any, offendingSymbol: any, line: any, column: any, msg: any, e: any): void;
    reportAmbiguity(recognizer: any, dfa: any, startIndex: any, stopIndex: any, exact: any, ambigAlts: any, configs: any): void;
    reportAttemptingFullContext(recognizer: any, dfa: any, startIndex: any, stopIndex: any, conflictingAlts: any, configs: any): void;
    reportContextSensitivity(recognizer: any, dfa: any, startIndex: any, stopIndex: any, prediction: any, configs: any): void;
}
declare const FailedPredicateException_base: {
    new (params: any): {
        message: any;
        recognizer: any;
        input: any;
        ctx: any;
        /**
         * The current {@link Token} when an error occurred. Since not all streams
         * support accessing symbols by index, we have to track the {@link Token}
         * instance itself
        */
        offendingToken: any;
        /**
         * Get the ATN state number the parser was in at the time the error
         * occurred. For {@link NoViableAltException} and
         * {@link LexerNoViableAltException} exceptions, this is the
         * {@link DecisionState} number. For others, it is the state whose outgoing
         * edge we couldn't match.
         */
        offendingState: any;
        /**
         * Gets the set of input symbols which could potentially follow the
         * previously matched symbol at the time this exception was thrown.
         *
         * <p>If the set of expected tokens is not known and could not be computed,
         * this method returns {@code null}.</p>
         *
         * @return The set of token types that could potentially follow the current
         * state in the ATN, or {@code null} if the information is not available.
         */
        getExpectedTokens(): any;
        toString(): any;
        name: string;
        stack?: string | undefined;
    };
    captureStackTrace(targetObject: object, constructorOpt?: Function | undefined): void;
    prepareStackTrace?: ((err: Error, stackTraces: NodeJS.CallSite[]) => any) | undefined;
    stackTraceLimit: number;
};
/**
 * A semantic predicate failed during validation. Validation of predicates
 * occurs when normally parsing the alternative just like matching a token.
 * Disambiguating predicate evaluation occurs when we test a predicate during
 * prediction.
 */
export class FailedPredicateException extends FailedPredicateException_base {
    constructor(recognizer: any, predicate: any, message: any);
    ruleIndex: any;
    predicateIndex: any;
    predicate: any;
}
export class Interval {
    constructor(start: any, stop: any);
    start: any;
    stop: any;
    clone(): {
        start: any;
        stop: any;
        clone(): any;
        contains(item: any): boolean;
        toString(): any;
        readonly length: number;
    };
    contains(item: any): boolean;
    toString(): any;
    get length(): number;
}
export class IntervalSet {
    intervals: any[] | null;
    readOnly: boolean;
    first(v: any): any;
    addOne(v: any): void;
    addRange(l: any, h: any): void;
    addInterval(toAdd: any): void;
    addSet(other: any): {
        intervals: any[] | null;
        readOnly: boolean;
        first(v: any): any;
        addOne(v: any): void;
        addRange(l: any, h: any): void;
        addInterval(toAdd: any): void;
        addSet(other: any): any;
        reduce(pos: any): void;
        complement(start: any, stop: any): any;
        contains(item: any): boolean;
        removeRange(toRemove: any): void;
        removeOne(value: any): void;
        toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
        toCharString(): string;
        toIndexString(): any;
        toTokenString(literalNames: any, symbolicNames: any): any;
        elementName(literalNames: any, symbolicNames: any, token: any): any;
        readonly length: any;
    };
    reduce(pos: any): void;
    complement(start: any, stop: any): {
        intervals: any[] | null;
        readOnly: boolean;
        first(v: any): any;
        addOne(v: any): void;
        addRange(l: any, h: any): void;
        addInterval(toAdd: any): void;
        addSet(other: any): any;
        reduce(pos: any): void;
        complement(start: any, stop: any): any;
        contains(item: any): boolean;
        removeRange(toRemove: any): void;
        removeOne(value: any): void;
        toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
        toCharString(): string;
        toIndexString(): any;
        toTokenString(literalNames: any, symbolicNames: any): any;
        elementName(literalNames: any, symbolicNames: any, token: any): any;
        readonly length: any;
    };
    contains(item: any): boolean;
    removeRange(toRemove: any): void;
    removeOne(value: any): void;
    toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
    toCharString(): string;
    toIndexString(): any;
    toTokenString(literalNames: any, symbolicNames: any): any;
    elementName(literalNames: any, symbolicNames: any, token: any): any;
    get length(): any;
}
export class LL1Analyzer {
    constructor(atn: any);
    atn: any;
    /**
     * Calculates the SLL(1) expected lookahead set for each outgoing transition
     * of an {@link ATNState}. The returned array has one element for each
     * outgoing transition in {@code s}. If the closure from transition
     * <em>i</em> leads to a semantic predicate before matching a symbol, the
     * element at index <em>i</em> of the result will be {@code null}.
     *
     * @param s the ATN state
     * @return the expected symbols for each outgoing transition of {@code s}.
     */
    getDecisionLookahead(s: any): ({
        intervals: any[] | null;
        readOnly: boolean;
        first(v: any): any;
        addOne(v: any): void;
        addRange(l: any, h: any): void;
        addInterval(toAdd: any): void;
        addSet(other: any): any;
        reduce(pos: any): void;
        complement(start: any, stop: any): any;
        contains(item: any): boolean;
        removeRange(toRemove: any): void;
        removeOne(value: any): void;
        toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
        toCharString(): string;
        toIndexString(): any;
        toTokenString(literalNames: any, symbolicNames: any): any;
        elementName(literalNames: any, symbolicNames: any, token: any): any;
        readonly length: any;
    } | null)[] | null;
    /**
     * Compute set of tokens that can follow {@code s} in the ATN in the
     * specified {@code ctx}.
     *
     * <p>If {@code ctx} is {@code null} and the end of the rule containing
     * {@code s} is reached, {@link Token//EPSILON} is added to the result set.
     * If {@code ctx} is not {@code null} and the end of the outermost rule is
     * reached, {@link Token//EOF} is added to the result set.</p>
     *
     * @param s the ATN state
     * @param stopState the ATN state to stop at. This can be a
     * {@link BlockEndState} to detect epsilon paths through a closure.
     * @param ctx the complete parser context, or {@code null} if the context
     * should be ignored
     *
     * @return The set of tokens that can follow {@code s} in the ATN in the
     * specified {@code ctx}.
     */
    LOOK(s: any, stopState: any, ctx: any): {
        intervals: any[] | null;
        readOnly: boolean;
        first(v: any): any;
        addOne(v: any): void;
        addRange(l: any, h: any): void;
        addInterval(toAdd: any): void;
        addSet(other: any): any;
        reduce(pos: any): void;
        complement(start: any, stop: any): any;
        contains(item: any): boolean;
        removeRange(toRemove: any): void;
        removeOne(value: any): void;
        toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
        toCharString(): string;
        toIndexString(): any;
        toTokenString(literalNames: any, symbolicNames: any): any;
        elementName(literalNames: any, symbolicNames: any, token: any): any;
        readonly length: any;
    };
    /**
     * Compute set of tokens that can follow {@code s} in the ATN in the
     * specified {@code ctx}.
     *
     * <p>If {@code ctx} is {@code null} and {@code stopState} or the end of the
     * rule containing {@code s} is reached, {@link Token//EPSILON} is added to
     * the result set. If {@code ctx} is not {@code null} and {@code addEOF} is
     * {@code true} and {@code stopState} or the end of the outermost rule is
     * reached, {@link Token//EOF} is added to the result set.</p>
     *
     * @param s the ATN state.
     * @param stopState the ATN state to stop at. This can be a
     * {@link BlockEndState} to detect epsilon paths through a closure.
     * @param ctx The outer context, or {@code null} if the outer context should
     * not be used.
     * @param look The result lookahead set.
     * @param lookBusy A set used for preventing epsilon closures in the ATN
     * from causing a stack overflow. Outside code should pass
     * {@code new CustomizedSet<ATNConfig>} for this argument.
     * @param calledRuleStack A set used for preventing left recursion in the
     * ATN from causing a stack overflow. Outside code should pass
     * {@code new BitSet()} for this argument.
     * @param seeThruPreds {@code true} to true semantic predicates as
     * implicitly {@code true} and "see through them", otherwise {@code false}
     * to treat semantic predicates as opaque and add {@link //HIT_PRED} to the
     * result if one is encountered.
     * @param addEOF Add {@link Token//EOF} to the result if the end of the
     * outermost context is reached. This parameter has no effect if {@code ctx}
     * is {@code null}.
     */
    _LOOK(s: any, stopState: any, ctx: any, look: any, lookBusy: any, calledRuleStack: any, seeThruPreds: any, addEOF: any): void;
}
declare const Lexer_base: {
    new (): {
        _listeners: {
            syntaxError(recognizer: any, offendingSymbol: any, line: any, column: any, msg: any, e: any): void;
            reportAmbiguity(recognizer: any, dfa: any, startIndex: any, stopIndex: any, exact: any, ambigAlts: any, configs: any): void;
            reportAttemptingFullContext(recognizer: any, dfa: any, startIndex: any, stopIndex: any, conflictingAlts: any, configs: any): void;
            reportContextSensitivity(recognizer: any, dfa: any, startIndex: any, stopIndex: any, prediction: any, configs: any): void;
        }[];
        _interp: any;
        _stateNumber: number;
        checkVersion(toolVersion: any): void;
        addErrorListener(listener: any): void;
        removeErrorListeners(): void;
        getLiteralNames(): any;
        getSymbolicNames(): any;
        getTokenNames(): any[];
        tokenNames: any[] | undefined;
        getTokenTypeMap(): any;
        /**
         * Get a map from rule names to rule indexes.
         * <p>Used for XPath and tree pattern compilation.</p>
         */
        getRuleIndexMap(): any;
        getTokenType(tokenName: any): any;
        getErrorHeader(e: any): string;
        /**
         * How should a token be displayed in an error message? The default
         * is to display just the text, but during development you might
         * want to have a lot of information spit out.  Override in that case
         * to use t.toString() (which, for CommonToken, dumps everything about
         * the token). This is better than forcing you to override a method in
         * your token objects because you don't have to go modify your lexer
         * so that it creates a new Java type.
         *
         * @deprecated This method is not called by the ANTLR 4 Runtime. Specific
         * implementations of {@link ANTLRErrorStrategy} may provide a similar
         * feature when necessary. For example, see
         * {@link DefaultErrorStrategy//getTokenErrorDisplay}.*/
        getTokenErrorDisplay(t: any): string;
        getErrorListenerDispatch(): {
            delegates: any;
            syntaxError(recognizer: any, offendingSymbol: any, line: any, column: any, msg: any, e: any): void;
            reportAmbiguity(recognizer: any, dfa: any, startIndex: any, stopIndex: any, exact: any, ambigAlts: any, configs: any): void;
            reportAttemptingFullContext(recognizer: any, dfa: any, startIndex: any, stopIndex: any, conflictingAlts: any, configs: any): void;
            reportContextSensitivity(recognizer: any, dfa: any, startIndex: any, stopIndex: any, prediction: any, configs: any): void;
        };
        /**
         * subclass needs to override these if there are sempreds or actions
         * that the ATN interp needs to execute
         */
        sempred(localctx: any, ruleIndex: any, actionIndex: any): boolean;
        precpred(localctx: any, precedence: any): boolean;
        readonly atn: any;
        state: number;
    };
    tokenTypeMapCache: {};
    ruleIndexMapCache: {};
};
/**
 * A lexer is recognizer that draws input symbols from a character stream.
 * lexer grammars result in a subclass of this object. A Lexer object
 * uses simplified match() and error recovery mechanisms in the interest of speed.
 */
export class Lexer extends Lexer_base {
    constructor(input: any);
    _input: any;
    _factory: {
        /**
         * Indicates whether {@link CommonToken//setText} should be called after
         * constructing tokens to explicitly set the text. This is useful for cases
         * where the input stream might not be able to provide arbitrary substrings
         * of text from the input after the lexer creates a token (e.g. the
         * implementation of {@link CharStream//getText} in
         * {@link UnbufferedCharStream} throws an
         * {@link UnsupportedOperationException}). Explicitly setting the token text
         * allows {@link Token//getText} to be called at any time regardless of the
         * input stream implementation.
         *
         * <p>
         * The default value is {@code false} to avoid the performance and memory
         * overhead of copying text for every token unless explicitly requested.</p>
         */
        copyText: any;
        create(source: any, type: any, text: any, channel: any, start: any, stop: any, line: any, column: any): {
            source: any;
            type: any;
            channel: any;
            start: any;
            stop: any;
            tokenIndex: number;
            line: any;
            column: any;
            /**
             * Constructs a new {@link CommonToken} as a copy of another {@link Token}.
             *
             * <p>
             * If {@code oldToken} is also a {@link CommonToken} instance, the newly
             * constructed token will share a reference to the {@link //text} field and
             * the {@link Pair} stored in {@link //source}. Otherwise, {@link //text} will
             * be assigned the result of calling {@link //getText}, and {@link //source}
             * will be constructed from the result of {@link Token//getTokenSource} and
             * {@link Token//getInputStream}.</p>
             *
             * @param oldToken The token to copy.
             */
            clone(): any;
            cloneWithType(type: any): any;
            toString(): string;
            text: any;
            _text: any;
            getTokenSource(): any;
            getInputStream(): any;
        };
        createThin(type: any, text: any): {
            source: any;
            type: any;
            channel: any;
            start: any;
            stop: any;
            tokenIndex: number;
            line: any;
            column: any;
            /**
             * Constructs a new {@link CommonToken} as a copy of another {@link Token}.
             *
             * <p>
             * If {@code oldToken} is also a {@link CommonToken} instance, the newly
             * constructed token will share a reference to the {@link //text} field and
             * the {@link Pair} stored in {@link //source}. Otherwise, {@link //text} will
             * be assigned the result of calling {@link //getText}, and {@link //source}
             * will be constructed from the result of {@link Token//getTokenSource} and
             * {@link Token//getInputStream}.</p>
             *
             * @param oldToken The token to copy.
             */
            clone(): any;
            cloneWithType(type: any): any;
            toString(): string;
            text: any;
            _text: any;
            getTokenSource(): any;
            getInputStream(): any;
        };
    };
    _tokenFactorySourcePair: any[];
    /**
     * The goal of all lexer rules/methods is to create a token object.
     * this is an instance variable as multiple rules may collaborate to
     * create a single token. nextToken will return this object after
     * matching lexer rule(s). If you subclass to allow multiple token
     * emissions, then set this to the last token to be matched or
     * something nonnull so that the auto token emit mechanism will not
     * emit another token.
     */
    _token: any;
    /**
     * What character index in the stream did the current token start at?
     * Needed, for example, to get the text for current token. Set at
     * the start of nextToken.
     */
    _tokenStartCharIndex: number;
    _tokenStartLine: number;
    _tokenStartColumn: number;
    _hitEOF: boolean;
    _channel: number;
    _type: number;
    _modeStack: any[];
    _mode: number;
    /**
     * You can set the text for the current token to override what is in
     * the input char buffer. Use setText() or can set this instance var.
     */
    _text: any;
    reset(): void;
    nextToken(): any;
    /**
     * Instruct the lexer to skip creating a token for current lexer rule
     * and look for another token. nextToken() knows to keep looking when
     * a lexer rule finishes with token set to SKIP_TOKEN. Recall that
     * if token==null at end of any token rule, it creates one for you
     * and emits it.
     */
    skip(): void;
    more(): void;
    mode(m: any): void;
    pushMode(m: any): void;
    popMode(): number;
    /**
     * By default does not support multiple emits per nextToken invocation
     * for efficiency reasons. Subclass and override this method, nextToken,
     * and getToken (to push tokens into a list and pull from that list
     * rather than a single variable as this implementation does).
     */
    emitToken(token: any): void;
    /**
     * The standard method called to automatically emit a token at the
     * outermost lexical rule. The token object should point into the
     * char buffer start..stop. If there is a text override in 'text',
     * use that to set the token's text. Override this method to emit
     * custom Token objects or provide a new factory.
     */
    emit(): {
        source: any;
        type: any;
        channel: any;
        start: any;
        stop: any;
        tokenIndex: number;
        line: any;
        column: any;
        /**
         * Constructs a new {@link CommonToken} as a copy of another {@link Token}.
         *
         * <p>
         * If {@code oldToken} is also a {@link CommonToken} instance, the newly
         * constructed token will share a reference to the {@link //text} field and
         * the {@link Pair} stored in {@link //source}. Otherwise, {@link //text} will
         * be assigned the result of calling {@link //getText}, and {@link //source}
         * will be constructed from the result of {@link Token//getTokenSource} and
         * {@link Token//getInputStream}.</p>
         *
         * @param oldToken The token to copy.
         */
        clone(): any;
        cloneWithType(type: any): any;
        toString(): string;
        text: any;
        _text: any;
        getTokenSource(): any;
        getInputStream(): any;
    };
    emitEOF(): {
        source: any;
        type: any;
        channel: any;
        start: any;
        stop: any;
        tokenIndex: number;
        line: any;
        column: any;
        /**
         * Constructs a new {@link CommonToken} as a copy of another {@link Token}.
         *
         * <p>
         * If {@code oldToken} is also a {@link CommonToken} instance, the newly
         * constructed token will share a reference to the {@link //text} field and
         * the {@link Pair} stored in {@link //source}. Otherwise, {@link //text} will
         * be assigned the result of calling {@link //getText}, and {@link //source}
         * will be constructed from the result of {@link Token//getTokenSource} and
         * {@link Token//getInputStream}.</p>
         *
         * @param oldToken The token to copy.
         */
        clone(): any;
        cloneWithType(type: any): any;
        toString(): string;
        text: any;
        _text: any;
        getTokenSource(): any;
        getInputStream(): any;
    };
    getCharIndex(): any;
    /**
     * Return a list of all Token objects in input char stream.
     * Forces load of all tokens. Does not include EOF token.
     */
    getAllTokens(): any[];
    notifyListeners(e: any): void;
    getErrorDisplay(s: any): string;
    getErrorDisplayForChar(c: any): any;
    getCharErrorDisplay(c: any): string;
    /**
     * Lexers can normally match any char in it's vocabulary after matching
     * a token, so do the easy thing and just kill a character and hope
     * it all works out. You can instead use the rule invocation stack
     * to do sophisticated error recovery if you are in a fragment rule.
     */
    recover(re: any): void;
    set inputStream(arg: any);
    get inputStream(): any;
    get sourceName(): any;
    set type(arg: number);
    get type(): number;
    set line(arg: any);
    get line(): any;
    set column(arg: any);
    get column(): any;
    set text(arg: any);
    get text(): any;
}
declare const LexerATNSimulator_base: {
    new (atn: any, sharedContextCache: any): {
        /**
         * The context cache maps all PredictionContext objects that are ==
         * to a single cached copy. This cache is shared across all contexts
         * in all ATNConfigs in all DFA states.  We rebuild each ATNConfigSet
         * to use only cached nodes/graphs in addDFAState(). We don't want to
         * fill this during closure() since there are lots of contexts that
         * pop up but are not used ever again. It also greatly slows down closure().
         *
         * <p>This cache makes a huge difference in memory and a little bit in speed.
         * For the Java grammar on java.*, it dropped the memory requirements
         * at the end from 25M to 16M. We don't store any of the full context
         * graphs in the DFA because they are limited to local context only,
         * but apparently there's a lot of repetition there as well. We optimize
         * the config contexts before storing the config set in the DFA states
         * by literally rebuilding them with cached subgraphs only.</p>
         *
         * <p>I tried a cache for use during closure operations, that was
         * whacked after each adaptivePredict(). It cost a little bit
         * more time I think and doesn't save on the overall footprint
         * so it's not worth the complexity.</p>
         */
        atn: any;
        sharedContextCache: any;
        getCachedContext(context: any): any;
    };
    ERROR: {
        stateNumber: any;
        configs: any;
        /**
         * {@code edges[symbol]} points to target of symbol. Shift up by 1 so (-1)
         * {@link Token//EOF} maps to {@code edges[0]}.
         */
        edges: any;
        isAcceptState: boolean;
        /**
         * if accept state, what ttype do we match or alt do we predict?
         * This is set to {@link ATN//INVALID_ALT_NUMBER} when {@link//predicates}
         * {@code !=null} or {@link //requiresFullContext}.
         */
        prediction: number;
        lexerActionExecutor: any;
        /**
         * Indicates that this state was created during SLL prediction that
         * discovered a conflict between the configurations in the state. Future
         * {@link ParserATNSimulator//execATN} invocations immediately jumped doing
         * full context prediction if this field is true.
         */
        requiresFullContext: boolean;
        /**
         * During SLL parsing, this is a list of predicates associated with the
         * ATN configurations of the DFA state. When we have predicates,
         * {@link //requiresFullContext} is {@code false} since full context
         * prediction evaluates predicates
         * on-the-fly. If this is not null, then {@link //prediction} is
         * {@link ATN//INVALID_ALT_NUMBER}.
         *
         * <p>We only use these for non-{@link //requiresFullContext} but
         * conflicting states. That
         * means we know from the context (it's $ or we don't dip into outer
         * context) that it's an ambiguity not a conflict.</p>
         *
         * <p>This list is computed by {@link
         * ParserATNSimulator//predicateDFAState}.</p>
         */
        predicates: any;
        /**
         * Get the set of all alts mentioned by all ATN configurations in this
         * DFA state.
         */
        getAltSet(): {
            data: {};
            hashFunction: any;
            equalsFunction: any;
            add(value: any): any;
            has(value: any): boolean;
            get(value: any): any;
            values(): any[];
            toString(): string;
            readonly length: any;
        } | null;
        /**
         * Two {@link DFAState} instances are equal if their ATN configuration sets
         * are the same. This method is used to see if a state already exists.
         *
         * <p>Because the number of alternatives and number of ATN configurations are
         * finite, there is a finite number of DFA states that can be processed.
         * This is necessary to show that the algorithm terminates.</p>
         *
         * <p>Cannot test the DFA state numbers here because in
         * {@link ParserATNSimulator//addDFAState} we need to know if any other state
         * exists that has this exact set of ATN configurations. The
         * {@link //stateNumber} is irrelevant.</p>
         */
        equals(other: any): any;
        toString(): string;
        hashCode(): number;
    };
};
export class LexerATNSimulator extends LexerATNSimulator_base {
    /**
     * When we hit an accept state in either the DFA or the ATN, we
     * have to notify the character stream to start buffering characters
     * via {@link IntStream//mark} and record the current state. The current sim state
     * includes the current index into the input, the current line,
     * and current character position in that line. Note that the Lexer is
     * tracking the starting line and characterization of the token. These
     * variables track the "state" of the simulator when it hits an accept state.
     *
     * <p>We track these variables separately for the DFA and ATN simulation
     * because the DFA simulation often has to fail over to the ATN
     * simulation. If the ATN simulation fails, we need the DFA to fall
     * back to its previously accepted state, if any. If the ATN succeeds,
     * then the ATN does the accept and the DFA simulator that invoked it
     * can simply return the predicted token type.</p>
     */
    constructor(recog: any, atn: any, decisionToDFA: any, sharedContextCache: any);
    decisionToDFA: any;
    recog: any;
    /**
     * The current token's starting index into the character stream.
     * Shared across DFA to ATN simulation in case the ATN fails and the
     * DFA did not have a previous accept state. In this case, we use the
     * ATN-generated exception object
     */
    startIndex: number;
    line: number;
    /**
     * The index of the character relative to the beginning of the line
     * 0..n-1
     */
    column: number;
    mode: number;
    /**
     * Used during DFA/ATN exec to record the most recent accept configuration
     * info
     */
    prevAccept: {
        reset(): void;
    };
    copyState(simulator: any): void;
    match(input: any, mode: any): any;
    reset(): void;
    matchATN(input: any): any;
    execATN(input: any, ds0: any): any;
    /**
     * Get an existing target state for an edge in the DFA. If the target state
     * for the edge has not yet been computed or is otherwise not available,
     * this method returns {@code null}.
     *
     * @param s The current DFA state
     * @param t The next input symbol
     * @return The existing target DFA state for the given input symbol
     * {@code t}, or {@code null} if the target state for this edge is not
     * already cached
     */
    getExistingTargetState(s: any, t: any): any;
    /**
     * Compute a target state for an edge in the DFA, and attempt to add the
     * computed state and corresponding edge to the DFA.
     *
     * @param input The input stream
     * @param s The current DFA state
     * @param t The next input symbol
     *
     * @return The computed target DFA state for the given input symbol
     * {@code t}. If {@code t} does not lead to a valid DFA state, this method
     * returns {@link //ERROR}.
     */
    computeTargetState(input: any, s: any, t: any): any;
    failOrAccept(prevAccept: any, input: any, reach: any, t: any): any;
    /**
     * Given a starting configuration set, figure out all ATN configurations
     * we can reach upon input {@code t}. Parameter {@code reach} is a return
     * parameter.
     */
    getReachableConfigSet(input: any, closure: any, reach: any, t: any): void;
    accept(input: any, lexerActionExecutor: any, startIndex: any, index: any, line: any, charPos: any): void;
    getReachableTarget(trans: any, t: any): any;
    computeStartState(input: any, p: any): {
        configLookup: {
            data: {};
            hashFunction: any;
            equalsFunction: any;
            add(value: any): any;
            has(value: any): boolean;
            get(value: any): any;
            values(): any[];
            toString(): string;
            readonly length: any;
        };
        /**
         * Indicates that this configuration set is part of a full context
         * LL prediction. It will be used to determine how to merge $. With SLL
         * it's a wildcard whereas it is not for LL context merge
         */
        fullCtx: any;
        /**
         * Indicates that the set of configurations is read-only. Do not
         * allow any code to manipulate the set; DFA states will point at
         * the sets and they must not change. This does not protect the other
         * fields; in particular, conflictingAlts is set after
         * we've made this readonly
         */
        readOnly: boolean;
        configs: any[];
        uniqueAlt: number;
        conflictingAlts: any;
        /**
         * Used in parser and lexer. In lexer, it indicates we hit a pred
         * while computing a closure operation. Don't make a DFA state from this
         */
        hasSemanticContext: boolean;
        dipsIntoOuterContext: boolean;
        cachedHashCode: number;
        /**
         * Adding a new config means merging contexts with existing configs for
         * {@code (s, i, pi, _)}, where {@code s} is the
         * {@link ATNConfig//state}, {@code i} is the {@link ATNConfig//alt}, and
         * {@code pi} is the {@link ATNConfig//semanticContext}. We use
         * {@code (s,i,pi)} as key.
         *
         * <p>This method updates {@link //dipsIntoOuterContext} and
         * {@link //hasSemanticContext} when necessary.</p>
         */
        add(config: any, mergeCache: any): boolean;
        getStates(): {
            data: {};
            hashFunction: any;
            equalsFunction: any;
            add(value: any): any;
            has(value: any): boolean;
            get(value: any): any;
            values(): any[];
            toString(): string;
            readonly length: any;
        };
        getPredicates(): any[];
        optimizeConfigs(interpreter: any): void;
        addAll(coll: any): boolean;
        equals(other: any): boolean;
        hashCode(): number;
        updateHashCode(hash: any): void;
        isEmpty(): boolean;
        contains(item: any): any;
        containsFast(item: any): any;
        clear(): void;
        setReadonly(readOnly: any): void;
        toString(): string;
        readonly items: any[];
        readonly length: number;
    };
    /**
     * Since the alternatives within any lexer decision are ordered by
     * preference, this method stops pursuing the closure as soon as an accept
     * state is reached. After the first accept state is reached by depth-first
     * search from {@code config}, all other (potentially reachable) states for
     * this rule would have a lower priority.
     *
     * @return {Boolean} {@code true} if an accept state is reached, otherwise
     * {@code false}.
     */
    closure(input: any, config: any, configs: any, currentAltReachedAcceptState: any, speculative: any, treatEofAsEpsilon: any): boolean;
    getEpsilonTarget(input: any, config: any, trans: any, configs: any, speculative: any, treatEofAsEpsilon: any): {
        lexerActionExecutor: any;
        passedThroughNonGreedyDecision: any;
        hashCodeForConfigSet: () => number;
        equalsForConfigSet: (other: any) => any;
        updateHashCode(hash: any): void;
        equals(other: any): any;
        checkNonGreedyDecision(source: any, target: any): any;
        state: any;
        alt: any;
        /**
         * The stack of invoking states leading to the rule/states associated
         * with this config.  We track only those contexts pushed during
         * execution of the ATN simulator
         */
        context: any;
        semanticContext: any;
        /**
         * We cannot execute predicates dependent upon local context unless
         * we know for sure we are in the correct context. Because there is
         * no way to do this efficiently, we simply cannot evaluate
         * dependent predicates unless we are in the rule that initially
         * invokes the ATN simulator.
         * closure() tracks the depth of how far we dip into the
         * outer context: depth &gt; 0.  Note that it may not be totally
         * accurate depth since I don't ever decrement
         */
        reachesIntoOuterContext: any;
        precedenceFilterSuppressed: any;
        checkContext(params: any, config: any): void;
        hashCode(): number;
        toString(): string;
    } | null;
    /**
     * Evaluate a predicate specified in the lexer.
     *
     * <p>If {@code speculative} is {@code true}, this method was called before
     * {@link //consume} for the matched character. This method should call
     * {@link //consume} before evaluating the predicate to ensure position
     * sensitive values, including {@link Lexer//getText}, {@link Lexer//getLine},
     * and {@link Lexer//getcolumn}, properly reflect the current
     * lexer state. This method should restore {@code input} and the simulator
     * to the original state before returning (i.e. undo the actions made by the
     * call to {@link //consume}.</p>
     *
     * @param input The input stream.
     * @param ruleIndex The rule containing the predicate.
     * @param predIndex The index of the predicate within the rule.
     * @param speculative {@code true} if the current index in {@code input} is
     * one character before the predicate's location.
     *
     * @return {@code true} if the specified predicate evaluates to
     * {@code true}.
     */
    evaluatePredicate(input: any, ruleIndex: any, predIndex: any, speculative: any): any;
    captureSimState(settings: any, input: any, dfaState: any): void;
    addDFAEdge(from_: any, tk: any, to: any, cfgs: any): any;
    /**
     * Add a new DFA state if there isn't one with this set of
     * configurations already. This method also detects the first
     * configuration containing an ATN rule stop state. Later, when
     * traversing the DFA, we will know which rule to accept.
     */
    addDFAState(configs: any): any;
    getDFA(mode: any): any;
    getText(input: any): any;
    consume(input: any): void;
    getTokenName(tt: any): string;
}
declare const NoViableAltException_base: {
    new (params: any): {
        message: any;
        recognizer: any;
        input: any;
        ctx: any;
        /**
         * The current {@link Token} when an error occurred. Since not all streams
         * support accessing symbols by index, we have to track the {@link Token}
         * instance itself
        */
        offendingToken: any;
        /**
         * Get the ATN state number the parser was in at the time the error
         * occurred. For {@link NoViableAltException} and
         * {@link LexerNoViableAltException} exceptions, this is the
         * {@link DecisionState} number. For others, it is the state whose outgoing
         * edge we couldn't match.
         */
        offendingState: any;
        /**
         * Gets the set of input symbols which could potentially follow the
         * previously matched symbol at the time this exception was thrown.
         *
         * <p>If the set of expected tokens is not known and could not be computed,
         * this method returns {@code null}.</p>
         *
         * @return The set of token types that could potentially follow the current
         * state in the ATN, or {@code null} if the information is not available.
         */
        getExpectedTokens(): any;
        toString(): any;
        name: string;
        stack?: string | undefined;
    };
    captureStackTrace(targetObject: object, constructorOpt?: Function | undefined): void;
    prepareStackTrace?: ((err: Error, stackTraces: NodeJS.CallSite[]) => any) | undefined;
    stackTraceLimit: number;
};
/**
 * Indicates that the parser could not decide which of two or more paths
 * to take based upon the remaining input. It tracks the starting token
 * of the offending input and also knows where the parser was
 * in the various paths when the error. Reported by reportNoViableAlternative()
 */
export class NoViableAltException extends NoViableAltException_base {
    constructor(recognizer: any, input: any, startToken: any, offendingToken: any, deadEndConfigs: any, ctx: any);
    deadEndConfigs: any;
    startToken: any;
}
export class ParseTreeListener {
    visitTerminal(node: any): void;
    visitErrorNode(node: any): void;
    enterEveryRule(node: any): void;
    exitEveryRule(node: any): void;
}
export class ParseTreeVisitor {
    visit(ctx: any): any;
    visitChildren(ctx: any): any;
    visitTerminal(node: any): void;
    visitErrorNode(node: any): void;
}
export class ParseTreeWalker {
    /**
     * Performs a walk on the given parse tree starting at the root and going down recursively
     * with depth-first search. On each node, {@link ParseTreeWalker//enterRule} is called before
     * recursively walking down into child nodes, then
     * {@link ParseTreeWalker//exitRule} is called after the recursive call to wind up.
     * @param listener The listener used by the walker to process grammar rules
     * @param t The parse tree to be walked on
     */
    walk(listener: any, t: any): void;
    /**
     * Enters a grammar rule by first triggering the generic event {@link ParseTreeListener//enterEveryRule}
     * then by triggering the event specific to the given parse tree node
     * @param listener The listener responding to the trigger events
     * @param r The grammar rule containing the rule context
     */
    enterRule(listener: any, r: any): void;
    /**
     * Exits a grammar rule by first triggering the event specific to the given parse tree node
     * then by triggering the generic event {@link ParseTreeListener//exitEveryRule}
     * @param listener The listener responding to the trigger events
     * @param r The grammar rule containing the rule context
     */
    exitRule(listener: any, r: any): void;
}
declare const Parser_base: {
    new (): {
        _listeners: {
            syntaxError(recognizer: any, offendingSymbol: any, line: any, column: any, msg: any, e: any): void;
            reportAmbiguity(recognizer: any, dfa: any, startIndex: any, stopIndex: any, exact: any, ambigAlts: any, configs: any): void;
            reportAttemptingFullContext(recognizer: any, dfa: any, startIndex: any, stopIndex: any, conflictingAlts: any, configs: any): void;
            reportContextSensitivity(recognizer: any, dfa: any, startIndex: any, stopIndex: any, prediction: any, configs: any): void;
        }[];
        _interp: any;
        _stateNumber: number;
        checkVersion(toolVersion: any): void;
        addErrorListener(listener: any): void;
        removeErrorListeners(): void;
        getLiteralNames(): any;
        getSymbolicNames(): any;
        getTokenNames(): any[];
        tokenNames: any[] | undefined;
        getTokenTypeMap(): any;
        /**
         * Get a map from rule names to rule indexes.
         * <p>Used for XPath and tree pattern compilation.</p>
         */
        getRuleIndexMap(): any;
        getTokenType(tokenName: any): any;
        getErrorHeader(e: any): string;
        /**
         * How should a token be displayed in an error message? The default
         * is to display just the text, but during development you might
         * want to have a lot of information spit out.  Override in that case
         * to use t.toString() (which, for CommonToken, dumps everything about
         * the token). This is better than forcing you to override a method in
         * your token objects because you don't have to go modify your lexer
         * so that it creates a new Java type.
         *
         * @deprecated This method is not called by the ANTLR 4 Runtime. Specific
         * implementations of {@link ANTLRErrorStrategy} may provide a similar
         * feature when necessary. For example, see
         * {@link DefaultErrorStrategy//getTokenErrorDisplay}.*/
        getTokenErrorDisplay(t: any): string;
        getErrorListenerDispatch(): {
            delegates: any;
            syntaxError(recognizer: any, offendingSymbol: any, line: any, column: any, msg: any, e: any): void;
            reportAmbiguity(recognizer: any, dfa: any, startIndex: any, stopIndex: any, exact: any, ambigAlts: any, configs: any): void;
            reportAttemptingFullContext(recognizer: any, dfa: any, startIndex: any, stopIndex: any, conflictingAlts: any, configs: any): void;
            reportContextSensitivity(recognizer: any, dfa: any, startIndex: any, stopIndex: any, prediction: any, configs: any): void;
        };
        /**
         * subclass needs to override these if there are sempreds or actions
         * that the ATN interp needs to execute
         */
        sempred(localctx: any, ruleIndex: any, actionIndex: any): boolean;
        precpred(localctx: any, precedence: any): boolean;
        readonly atn: any;
        state: number;
    };
    tokenTypeMapCache: {};
    ruleIndexMapCache: {};
};
export class Parser extends Parser_base {
    /**
     * this is all the parsing support code essentially; most of it is error
     * recovery stuff.
     */
    constructor(input: any);
    _input: any;
    /**
     * The error handling strategy for the parser. The default value is a new
     * instance of {@link DefaultErrorStrategy}.
     */
    _errHandler: {
        /**
         * Indicates whether the error strategy is currently "recovering from an
         * error". This is used to suppress reporting multiple error messages while
         * attempting to recover from a detected syntax error.
         *
         * @see //inErrorRecoveryMode
         */
        errorRecoveryMode: boolean;
        /**
         * The index into the input stream where the last error occurred.
         * This is used to prevent infinite loops where an error is found
         * but no token is consumed during recovery...another error is found,
         * ad nauseum. This is a failsafe mechanism to guarantee that at least
         * one token/tree node is consumed for two errors.
         */
        lastErrorIndex: number;
        lastErrorStates: any[] | null;
        nextTokensContext: any;
        nextTokenState: number;
        /**
         * <p>The default implementation simply calls {@link //endErrorCondition} to
         * ensure that the handler is not in error recovery mode.</p>
         */
        reset(recognizer: any): void;
        /**
         * This method is called to enter error recovery mode when a recognition
         * exception is reported.
         *
         * @param recognizer the parser instance
         */
        beginErrorCondition(recognizer: any): void;
        inErrorRecoveryMode(recognizer: any): boolean;
        /**
         * This method is called to leave error recovery mode after recovering from
         * a recognition exception.
         * @param recognizer
         */
        endErrorCondition(recognizer: any): void;
        /**
         * {@inheritDoc}
         * <p>The default implementation simply calls {@link //endErrorCondition}.</p>
         */
        reportMatch(recognizer: any): void;
        /**
         * {@inheritDoc}
         *
         * <p>The default implementation returns immediately if the handler is already
         * in error recovery mode. Otherwise, it calls {@link //beginErrorCondition}
         * and dispatches the reporting task based on the runtime type of {@code e}
         * according to the following table.</p>
         *
         * <ul>
         * <li>{@link NoViableAltException}: Dispatches the call to
         * {@link //reportNoViableAlternative}</li>
         * <li>{@link InputMismatchException}: Dispatches the call to
         * {@link //reportInputMismatch}</li>
         * <li>{@link FailedPredicateException}: Dispatches the call to
         * {@link //reportFailedPredicate}</li>
         * <li>All other types: calls {@link Parser//notifyErrorListeners} to report
         * the exception</li>
         * </ul>
         */
        reportError(recognizer: any, e: any): void;
        /**
         *
         * {@inheritDoc}
         *
         * <p>The default implementation resynchronizes the parser by consuming tokens
         * until we find one in the resynchronization set--loosely the set of tokens
         * that can follow the current rule.</p>
         *
         */
        recover(recognizer: any, e: any): void;
        /**
         * The default implementation of {@link ANTLRErrorStrategy//sync} makes sure
         * that the current lookahead symbol is consistent with what were expecting
         * at this point in the ATN. You can call this anytime but ANTLR only
         * generates code to check before subrules/loops and each iteration.
         *
         * <p>Implements Jim Idle's magic sync mechanism in closures and optional
         * subrules. E.g.,</p>
         *
         * <pre>
         * a : sync ( stuff sync )* ;
         * sync : {consume to what can follow sync} ;
         * </pre>
         *
         * At the start of a sub rule upon error, {@link //sync} performs single
         * token deletion, if possible. If it can't do that, it bails on the current
         * rule and uses the default error recovery, which consumes until the
         * resynchronization set of the current rule.
         *
         * <p>If the sub rule is optional ({@code (...)?}, {@code (...)*}, or block
         * with an empty alternative), then the expected set includes what follows
         * the subrule.</p>
         *
         * <p>During loop iteration, it consumes until it sees a token that can start a
         * sub rule or what follows loop. Yes, that is pretty aggressive. We opt to
         * stay in the loop as long as possible.</p>
         *
         * <p><strong>ORIGINS</strong></p>
         *
         * <p>Previous versions of ANTLR did a poor job of their recovery within loops.
         * A single mismatch token or missing token would force the parser to bail
         * out of the entire rules surrounding the loop. So, for rule</p>
         *
         * <pre>
         * classDef : 'class' ID '{' member* '}'
         * </pre>
         *
         * input with an extra token between members would force the parser to
         * consume until it found the next class definition rather than the next
         * member definition of the current class.
         *
         * <p>This functionality cost a little bit of effort because the parser has to
         * compare token set at the start of the loop and at each iteration. If for
         * some reason speed is suffering for you, you can turn off this
         * functionality by simply overriding this method as a blank { }.</p>
         *
         */
        sync(recognizer: any): void;
        nextTokensState: any;
        /**
         * This is called by {@link //reportError} when the exception is a
         * {@link NoViableAltException}.
         *
         * @see //reportError
         *
         * @param recognizer the parser instance
         * @param e the recognition exception
         */
        reportNoViableAlternative(recognizer: any, e: any): void;
        /**
         * This is called by {@link //reportError} when the exception is an
         * {@link InputMismatchException}.
         *
         * @see //reportError
         *
         * @param recognizer the parser instance
         * @param e the recognition exception
         */
        reportInputMismatch(recognizer: any, e: any): void;
        /**
         * This is called by {@link //reportError} when the exception is a
         * {@link FailedPredicateException}.
         *
         * @see //reportError
         *
         * @param recognizer the parser instance
         * @param e the recognition exception
         */
        reportFailedPredicate(recognizer: any, e: any): void;
        /**
         * This method is called to report a syntax error which requires the removal
         * of a token from the input stream. At the time this method is called, the
         * erroneous symbol is current {@code LT(1)} symbol and has not yet been
         * removed from the input stream. When this method returns,
         * {@code recognizer} is in error recovery mode.
         *
         * <p>This method is called when {@link //singleTokenDeletion} identifies
         * single-token deletion as a viable recovery strategy for a mismatched
         * input error.</p>
         *
         * <p>The default implementation simply returns if the handler is already in
         * error recovery mode. Otherwise, it calls {@link //beginErrorCondition} to
         * enter error recovery mode, followed by calling
         * {@link Parser//notifyErrorListeners}.</p>
         *
         * @param recognizer the parser instance
         *
         */
        reportUnwantedToken(recognizer: any): void;
        /**
         * This method is called to report a syntax error which requires the
         * insertion of a missing token into the input stream. At the time this
         * method is called, the missing token has not yet been inserted. When this
         * method returns, {@code recognizer} is in error recovery mode.
         *
         * <p>This method is called when {@link //singleTokenInsertion} identifies
         * single-token insertion as a viable recovery strategy for a mismatched
         * input error.</p>
         *
         * <p>The default implementation simply returns if the handler is already in
         * error recovery mode. Otherwise, it calls {@link //beginErrorCondition} to
         * enter error recovery mode, followed by calling
         * {@link Parser//notifyErrorListeners}.</p>
         *
         * @param recognizer the parser instance
         */
        reportMissingToken(recognizer: any): void;
        /**
         * <p>The default implementation attempts to recover from the mismatched input
         * by using single token insertion and deletion as described below. If the
         * recovery attempt fails, this method throws an
         * {@link InputMismatchException}.</p>
         *
         * <p><strong>EXTRA TOKEN</strong> (single token deletion)</p>
         *
         * <p>{@code LA(1)} is not what we are looking for. If {@code LA(2)} has the
         * right token, however, then assume {@code LA(1)} is some extra spurious
         * token and delete it. Then consume and return the next token (which was
         * the {@code LA(2)} token) as the successful result of the match operation.</p>
         *
         * <p>This recovery strategy is implemented by {@link
            * //singleTokenDeletion}.</p>
         *
         * <p><strong>MISSING TOKEN</strong> (single token insertion)</p>
         *
         * <p>If current token (at {@code LA(1)}) is consistent with what could come
         * after the expected {@code LA(1)} token, then assume the token is missing
         * and use the parser's {@link TokenFactory} to create it on the fly. The
         * "insertion" is performed by returning the created token as the successful
         * result of the match operation.</p>
         *
         * <p>This recovery strategy is implemented by {@link
            * //singleTokenInsertion}.</p>
         *
         * <p><strong>EXAMPLE</strong></p>
         *
         * <p>For example, Input {@code i=(3;} is clearly missing the {@code ')'}. When
         * the parser returns from the nested call to {@code expr}, it will have
         * call chain:</p>
         *
         * <pre>
         * stat &rarr; expr &rarr; atom
         * </pre>
         *
         * and it will be trying to match the {@code ')'} at this point in the
         * derivation:
         *
         * <pre>
         * =&gt; ID '=' '(' INT ')' ('+' atom)* ';'
         * ^
         * </pre>
         *
         * The attempt to match {@code ')'} will fail when it sees {@code ';'} and
         * call {@link //recoverInline}. To recover, it sees that {@code LA(1)==';'}
         * is in the set of tokens that can follow the {@code ')'} token reference
         * in rule {@code atom}. It can assume that you forgot the {@code ')'}.
         */
        recoverInline(recognizer: any): any;
        /**
         * This method implements the single-token insertion inline error recovery
         * strategy. It is called by {@link //recoverInline} if the single-token
         * deletion strategy fails to recover from the mismatched input. If this
         * method returns {@code true}, {@code recognizer} will be in error recovery
         * mode.
         *
         * <p>This method determines whether or not single-token insertion is viable by
         * checking if the {@code LA(1)} input symbol could be successfully matched
         * if it were instead the {@code LA(2)} symbol. If this method returns
         * {@code true}, the caller is responsible for creating and inserting a
         * token with the correct type to produce this behavior.</p>
         *
         * @param recognizer the parser instance
         * @return {@code true} if single-token insertion is a viable recovery
         * strategy for the current mismatched input, otherwise {@code false}
         */
        singleTokenInsertion(recognizer: any): any;
        /**
         * This method implements the single-token deletion inline error recovery
         * strategy. It is called by {@link //recoverInline} to attempt to recover
         * from mismatched input. If this method returns null, the parser and error
         * handler state will not have changed. If this method returns non-null,
         * {@code recognizer} will <em>not</em> be in error recovery mode since the
         * returned token was a successful match.
         *
         * <p>If the single-token deletion is successful, this method calls
         * {@link //reportUnwantedToken} to report the error, followed by
         * {@link Parser//consume} to actually "delete" the extraneous token. Then,
         * before returning {@link //reportMatch} is called to signal a successful
         * match.</p>
         *
         * @param recognizer the parser instance
         * @return the successfully matched {@link Token} instance if single-token
         * deletion successfully recovers from the mismatched input, otherwise
         * {@code null}
         */
        singleTokenDeletion(recognizer: any): any;
        /**
         * Conjure up a missing token during error recovery.
         *
         * The recognizer attempts to recover from single missing
         * symbols. But, actions might refer to that missing symbol.
         * For example, x=ID {f($x);}. The action clearly assumes
         * that there has been an identifier matched previously and that
         * $x points at that token. If that token is missing, but
         * the next token in the stream is what we want we assume that
         * this token is missing and we keep going. Because we
         * have to return some token to replace the missing token,
         * we have to conjure one up. This method gives the user control
         * over the tokens returned for missing tokens. Mostly,
         * you will want to create something special for identifier
         * tokens. For literals such as '{' and ',', the default
         * action in the parser or tree parser works. It simply creates
         * a CommonToken of the appropriate type. The text will be the token.
         * If you change what tokens must be created by the lexer,
         * override this method to create the appropriate tokens.
         *
         */
        getMissingSymbol(recognizer: any): any;
        getExpectedTokens(recognizer: any): any;
        /**
         * How should a token be displayed in an error message? The default
         * is to display just the text, but during development you might
         * want to have a lot of information spit out. Override in that case
         * to use t.toString() (which, for CommonToken, dumps everything about
         * the token). This is better than forcing you to override a method in
         * your token objects because you don't have to go modify your lexer
         * so that it creates a new Java type.
         */
        getTokenErrorDisplay(t: any): string;
        escapeWSAndQuote(s: any): string;
        /**
         * Compute the error recovery set for the current rule. During
         * rule invocation, the parser pushes the set of tokens that can
         * follow that rule reference on the stack; this amounts to
         * computing FIRST of what follows the rule reference in the
         * enclosing rule. See LinearApproximator.FIRST().
         * This local follow set only includes tokens
         * from within the rule; i.e., the FIRST computation done by
         * ANTLR stops at the end of a rule.
         *
         * EXAMPLE
         *
         * When you find a "no viable alt exception", the input is not
         * consistent with any of the alternatives for rule r. The best
         * thing to do is to consume tokens until you see something that
         * can legally follow a call to r//or* any rule that called r.
         * You don't want the exact set of viable next tokens because the
         * input might just be missing a token--you might consume the
         * rest of the input looking for one of the missing tokens.
         *
         * Consider grammar:
         *
         * a : '[' b ']'
         * | '(' b ')'
         * ;
         * b : c '^' INT ;
         * c : ID
         * | INT
         * ;
         *
         * At each rule invocation, the set of tokens that could follow
         * that rule is pushed on a stack. Here are the various
         * context-sensitive follow sets:
         *
         * FOLLOW(b1_in_a) = FIRST(']') = ']'
         * FOLLOW(b2_in_a) = FIRST(')') = ')'
         * FOLLOW(c_in_b) = FIRST('^') = '^'
         *
         * Upon erroneous input "[]", the call chain is
         *
         * a -> b -> c
         *
         * and, hence, the follow context stack is:
         *
         * depth follow set start of rule execution
         * 0 <EOF> a (from main())
         * 1 ']' b
         * 2 '^' c
         *
         * Notice that ')' is not included, because b would have to have
         * been called from a different context in rule a for ')' to be
         * included.
         *
         * For error recovery, we cannot consider FOLLOW(c)
         * (context-sensitive or otherwise). We need the combined set of
         * all context-sensitive FOLLOW sets--the set of all tokens that
         * could follow any reference in the call chain. We need to
         * resync to one of those tokens. Note that FOLLOW(c)='^' and if
         * we resync'd to that token, we'd consume until EOF. We need to
         * sync to context-sensitive FOLLOWs for a, b, and c: {']','^'}.
         * In this case, for input "[]", LA(1) is ']' and in the set, so we would
         * not consume anything. After printing an error, rule c would
         * return normally. Rule b would not find the required '^' though.
         * At this point, it gets a mismatched token error and throws an
         * exception (since LA(1) is not in the viable following token
         * set). The rule exception handler tries to recover, but finds
         * the same recovery set and doesn't consume anything. Rule b
         * exits normally returning to rule a. Now it finds the ']' (and
         * with the successful match exits errorRecovery mode).
         *
         * So, you can see that the parser walks up the call chain looking
         * for the token that was a member of the recovery set.
         *
         * Errors are not generated in errorRecovery mode.
         *
         * ANTLR's error recovery mechanism is based upon original ideas:
         *
         * "Algorithms + Data Structures = Programs" by Niklaus Wirth
         *
         * and
         *
         * "A note on error recovery in recursive descent parsers":
         * http://portal.acm.org/citation.cfm?id=947902.947905
         *
         * Later, Josef Grosch had some good ideas:
         *
         * "Efficient and Comfortable Error Recovery in Recursive Descent
         * Parsers":
         * ftp://www.cocolab.com/products/cocktail/doca4.ps/ell.ps.zip
         *
         * Like Grosch I implement context-sensitive FOLLOW sets that are combined
         * at run-time upon error to avoid overhead during parsing.
         */
        getErrorRecoverySet(recognizer: any): {
            intervals: any[] | null;
            readOnly: boolean;
            first(v: any): any;
            addOne(v: any): void;
            addRange(l: any, h: any): void;
            addInterval(toAdd: any): void;
            addSet(other: any): any;
            reduce(pos: any): void;
            complement(start: any, stop: any): any;
            contains(item: any): boolean;
            removeRange(toRemove: any): void;
            removeOne(value: any): void;
            toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
            toCharString(): string;
            toIndexString(): any;
            toTokenString(literalNames: any, symbolicNames: any): any;
            elementName(literalNames: any, symbolicNames: any, token: any): any;
            readonly length: any;
        };
        consumeUntil(recognizer: any, set: any): void;
    };
    _precedenceStack: number[];
    /**
     * The {@link ParserRuleContext} object for the currently executing rule.
     * this is always non-null during the parsing process.
     */
    _ctx: any;
    /**
     * Specifies whether or not the parser should construct a parse tree during
     * the parsing process. The default value is {@code true}.
     */
    buildParseTrees: boolean;
    /**
     * When {@link //setTrace}{@code (true)} is called, a reference to the
     * {@link TraceListener} is stored here so it can be easily removed in a
     * later call to {@link //setTrace}{@code (false)}. The listener itself is
     * implemented as a parser listener so this field is not directly used by
     * other parser methods.
     */
    _tracer: {
        parser: any;
        enterEveryRule(ctx: any): void;
        visitTerminal(node: any): void;
        exitEveryRule(ctx: any): void;
        visitErrorNode(node: any): void;
    } | null;
    /**
     * The list of {@link ParseTreeListener} listeners registered to receive
     * events during the parse.
     */
    _parseListeners: any[] | null;
    /**
     * The number of syntax errors reported during parsing. this value is
     * incremented each time {@link //notifyErrorListeners} is called.
     */
    _syntaxErrors: number;
    reset(): void;
    /**
     * Match current input symbol against {@code ttype}. If the symbol type
     * matches, {@link ANTLRErrorStrategy//reportMatch} and {@link //consume} are
     * called to complete the match process.
     *
     * <p>If the symbol type does not match,
     * {@link ANTLRErrorStrategy//recoverInline} is called on the current error
     * strategy to attempt recovery. If {@link //buildParseTree} is
     * {@code true} and the token index of the symbol returned by
     * {@link ANTLRErrorStrategy//recoverInline} is -1, the symbol is added to
     * the parse tree by calling {@link ParserRuleContext//addErrorNode}.</p>
     *
     * @param ttype the token type to match
     * @return the matched symbol
     * @throws RecognitionException if the current input symbol did not match
     * {@code ttype} and the error strategy could not recover from the
     * mismatched symbol
     */
    match(ttype: any): any;
    /**
     * Match current input symbol as a wildcard. If the symbol type matches
     * (i.e. has a value greater than 0), {@link ANTLRErrorStrategy//reportMatch}
     * and {@link //consume} are called to complete the match process.
     *
     * <p>If the symbol type does not match,
     * {@link ANTLRErrorStrategy//recoverInline} is called on the current error
     * strategy to attempt recovery. If {@link //buildParseTree} is
     * {@code true} and the token index of the symbol returned by
     * {@link ANTLRErrorStrategy//recoverInline} is -1, the symbol is added to
     * the parse tree by calling {@link ParserRuleContext//addErrorNode}.</p>
     *
     * @return the matched symbol
     * @throws RecognitionException if the current input symbol did not match
     * a wildcard and the error strategy could not recover from the mismatched
     * symbol
     */
    matchWildcard(): any;
    getParseListeners(): any[];
    /**
     * Registers {@code listener} to receive events during the parsing process.
     *
     * <p>To support output-preserving grammar transformations (including but not
     * limited to left-recursion removal, automated left-factoring, and
     * optimized code generation), calls to listener methods during the parse
     * may differ substantially from calls made by
     * {@link ParseTreeWalker//DEFAULT} used after the parse is complete. In
     * particular, rule entry and exit events may occur in a different order
     * during the parse than after the parser. In addition, calls to certain
     * rule entry methods may be omitted.</p>
     *
     * <p>With the following specific exceptions, calls to listener events are
     * <em>deterministic</em>, i.e. for identical input the calls to listener
     * methods will be the same.</p>
     *
     * <ul>
     * <li>Alterations to the grammar used to generate code may change the
     * behavior of the listener calls.</li>
     * <li>Alterations to the command line options passed to ANTLR 4 when
     * generating the parser may change the behavior of the listener calls.</li>
     * <li>Changing the version of the ANTLR Tool used to generate the parser
     * may change the behavior of the listener calls.</li>
     * </ul>
     *
     * @param listener the listener to add
     *
     * @throws NullPointerException if {@code} listener is {@code null}
     */
    addParseListener(listener: any): void;
    /**
     * Remove {@code listener} from the list of parse listeners.
     *
     * <p>If {@code listener} is {@code null} or has not been added as a parse
     * listener, this method does nothing.</p>
     * @param listener the listener to remove
     */
    removeParseListener(listener: any): void;
    removeParseListeners(): void;
    triggerEnterRuleEvent(): void;
    /**
     * Notify any parse listeners of an exit rule event.
     * @see //addParseListener
     */
    triggerExitRuleEvent(): void;
    getTokenFactory(): any;
    setTokenFactory(factory: any): void;
    /**
     * The ATN with bypass alternatives is expensive to create so we create it
     * lazily.
     *
     * @throws UnsupportedOperationException if the current parser does not
     * implement the {@link //getSerializedATN()} method.
     */
    getATNWithBypassAlts(): any;
    getInputStream(): any;
    setInputStream(input: any): void;
    getTokenStream(): any;
    setTokenStream(input: any): void;
    /**
     * Match needs to return the current input symbol, which gets put
     * into the label for the associated token ref; e.g., x=ID.
     */
    getCurrentToken(): any;
    notifyErrorListeners(msg: any, offendingToken: any, err: any): void;
    /**
     * Consume and return the {@linkplain //getCurrentToken current symbol}.
     *
     * <p>E.g., given the following input with {@code A} being the current
     * lookahead symbol, this function moves the cursor to {@code B} and returns
     * {@code A}.</p>
     *
     * <pre>
     * A B
     * ^
     * </pre>
     *
     * If the parser is not in error recovery mode, the consumed symbol is added
     * to the parse tree using {@link ParserRuleContext//addChild(Token)}, and
     * {@link ParseTreeListener//visitTerminal} is called on any parse listeners.
     * If the parser <em>is</em> in error recovery mode, the consumed symbol is
     * added to the parse tree using
     * {@link ParserRuleContext//addErrorNode(Token)}, and
     * {@link ParseTreeListener//visitErrorNode} is called on any parse
     * listeners.
     */
    consume(): any;
    addContextToParseTree(): void;
    /**
     * Always called by generated parsers upon entry to a rule. Access field
     * {@link //_ctx} get the current context.
     */
    enterRule(localctx: any, state: any, ruleIndex: any): void;
    exitRule(): void;
    enterOuterAlt(localctx: any, altNum: any): void;
    /**
     * Get the precedence level for the top-most precedence rule.
     *
     * @return The precedence level for the top-most precedence rule, or -1 if
     * the parser context is not nested within a precedence rule.
     */
    getPrecedence(): number;
    enterRecursionRule(localctx: any, state: any, ruleIndex: any, precedence: any): void;
    pushNewRecursionContext(localctx: any, state: any, ruleIndex: any): void;
    unrollRecursionContexts(parentCtx: any): void;
    getInvokingContext(ruleIndex: any): any;
    inContext(context: any): boolean;
    /**
     * Checks whether or not {@code symbol} can follow the current state in the
     * ATN. The behavior of this method is equivalent to the following, but is
     * implemented such that the complete context-sensitive follow set does not
     * need to be explicitly constructed.
     *
     * <pre>
     * return getExpectedTokens().contains(symbol);
     * </pre>
     *
     * @param symbol the symbol type to check
     * @return {@code true} if {@code symbol} can follow the current state in
     * the ATN, otherwise {@code false}.
     */
    isExpectedToken(symbol: any): any;
    /**
     * Computes the set of input symbols which could follow the current parser
     * state and context, as given by {@link //getState} and {@link //getContext},
     * respectively.
     *
     * @see ATN//getExpectedTokens(int, RuleContext)
     */
    getExpectedTokens(): any;
    getExpectedTokensWithinCurrentRule(): any;
    getRuleIndex(ruleName: any): any;
    /**
     * Return List&lt;String&gt; of the rule names in your parser instance
     * leading up to a call to the current rule. You could override if
     * you want more details such as the file/line info of where
     * in the ATN a rule is invoked.
     *
     * this is very useful for error messages.
     */
    getRuleInvocationStack(p: any): any[];
    getDFAStrings(): any;
    dumpDFA(): void;
    getSourceName(): any;
    /**
     * During a parse is sometimes useful to listen in on the rule entry and exit
     * events as well as token matches. this is for quick and dirty debugging.
     */
    setTrace(trace: any): void;
}
declare const ParserATNSimulator_base: {
    new (atn: any, sharedContextCache: any): {
        /**
         * The context cache maps all PredictionContext objects that are ==
         * to a single cached copy. This cache is shared across all contexts
         * in all ATNConfigs in all DFA states.  We rebuild each ATNConfigSet
         * to use only cached nodes/graphs in addDFAState(). We don't want to
         * fill this during closure() since there are lots of contexts that
         * pop up but are not used ever again. It also greatly slows down closure().
         *
         * <p>This cache makes a huge difference in memory and a little bit in speed.
         * For the Java grammar on java.*, it dropped the memory requirements
         * at the end from 25M to 16M. We don't store any of the full context
         * graphs in the DFA because they are limited to local context only,
         * but apparently there's a lot of repetition there as well. We optimize
         * the config contexts before storing the config set in the DFA states
         * by literally rebuilding them with cached subgraphs only.</p>
         *
         * <p>I tried a cache for use during closure operations, that was
         * whacked after each adaptivePredict(). It cost a little bit
         * more time I think and doesn't save on the overall footprint
         * so it's not worth the complexity.</p>
         */
        atn: any;
        sharedContextCache: any;
        getCachedContext(context: any): any;
    };
    ERROR: {
        stateNumber: any;
        configs: any;
        /**
         * {@code edges[symbol]} points to target of symbol. Shift up by 1 so (-1)
         * {@link Token//EOF} maps to {@code edges[0]}.
         */
        edges: any;
        isAcceptState: boolean;
        /**
         * if accept state, what ttype do we match or alt do we predict?
         * This is set to {@link ATN//INVALID_ALT_NUMBER} when {@link//predicates}
         * {@code !=null} or {@link //requiresFullContext}.
         */
        prediction: number;
        lexerActionExecutor: any;
        /**
         * Indicates that this state was created during SLL prediction that
         * discovered a conflict between the configurations in the state. Future
         * {@link ParserATNSimulator//execATN} invocations immediately jumped doing
         * full context prediction if this field is true.
         */
        requiresFullContext: boolean;
        /**
         * During SLL parsing, this is a list of predicates associated with the
         * ATN configurations of the DFA state. When we have predicates,
         * {@link //requiresFullContext} is {@code false} since full context
         * prediction evaluates predicates
         * on-the-fly. If this is not null, then {@link //prediction} is
         * {@link ATN//INVALID_ALT_NUMBER}.
         *
         * <p>We only use these for non-{@link //requiresFullContext} but
         * conflicting states. That
         * means we know from the context (it's $ or we don't dip into outer
         * context) that it's an ambiguity not a conflict.</p>
         *
         * <p>This list is computed by {@link
         * ParserATNSimulator//predicateDFAState}.</p>
         */
        predicates: any;
        /**
         * Get the set of all alts mentioned by all ATN configurations in this
         * DFA state.
         */
        getAltSet(): {
            data: {};
            hashFunction: any;
            equalsFunction: any;
            add(value: any): any;
            has(value: any): boolean;
            get(value: any): any;
            values(): any[];
            toString(): string;
            readonly length: any;
        } | null;
        /**
         * Two {@link DFAState} instances are equal if their ATN configuration sets
         * are the same. This method is used to see if a state already exists.
         *
         * <p>Because the number of alternatives and number of ATN configurations are
         * finite, there is a finite number of DFA states that can be processed.
         * This is necessary to show that the algorithm terminates.</p>
         *
         * <p>Cannot test the DFA state numbers here because in
         * {@link ParserATNSimulator//addDFAState} we need to know if any other state
         * exists that has this exact set of ATN configurations. The
         * {@link //stateNumber} is irrelevant.</p>
         */
        equals(other: any): any;
        toString(): string;
        hashCode(): number;
    };
};
/**
 * The embodiment of the adaptive LL(*), ALL(*), parsing strategy.
 *
 * <p>
 * The basic complexity of the adaptive strategy makes it harder to understand.
 * We begin with ATN simulation to build paths in a DFA. Subsequent prediction
 * requests go through the DFA first. If they reach a state without an edge for
 * the current symbol, the algorithm fails over to the ATN simulation to
 * complete the DFA path for the current input (until it finds a conflict state
 * or uniquely predicting state).</p>
 *
 * <p>
 * All of that is done without using the outer context because we want to create
 * a DFA that is not dependent upon the rule invocation stack when we do a
 * prediction. One DFA works in all contexts. We avoid using context not
 * necessarily because it's slower, although it can be, but because of the DFA
 * caching problem. The closure routine only considers the rule invocation stack
 * created during prediction beginning in the decision rule. For example, if
 * prediction occurs without invoking another rule's ATN, there are no context
 * stacks in the configurations. When lack of context leads to a conflict, we
 * don't know if it's an ambiguity or a weakness in the strong LL(*) parsing
 * strategy (versus full LL(*)).</p>
 *
 * <p>
 * When SLL yields a configuration set with conflict, we rewind the input and
 * retry the ATN simulation, this time using full outer context without adding
 * to the DFA. Configuration context stacks will be the full invocation stacks
 * from the start rule. If we get a conflict using full context, then we can
 * definitively say we have a true ambiguity for that input sequence. If we
 * don't get a conflict, it implies that the decision is sensitive to the outer
 * context. (It is not context-sensitive in the sense of context-sensitive
 * grammars.)</p>
 *
 * <p>
 * The next time we reach this DFA state with an SLL conflict, through DFA
 * simulation, we will again retry the ATN simulation using full context mode.
 * This is slow because we can't save the results and have to "interpret" the
 * ATN each time we get that input.</p>
 *
 * <p>
 * <strong>CACHING FULL CONTEXT PREDICTIONS</strong></p>
 *
 * <p>
 * We could cache results from full context to predicted alternative easily and
 * that saves a lot of time but doesn't work in presence of predicates. The set
 * of visible predicates from the ATN start state changes depending on the
 * context, because closure can fall off the end of a rule. I tried to cache
 * tuples (stack context, semantic context, predicted alt) but it was slower
 * than interpreting and much more complicated. Also required a huge amount of
 * memory. The goal is not to create the world's fastest parser anyway. I'd like
 * to keep this algorithm simple. By launching multiple threads, we can improve
 * the speed of parsing across a large number of files.</p>
 *
 * <p>
 * There is no strict ordering between the amount of input used by SLL vs LL,
 * which makes it really hard to build a cache for full context. Let's say that
 * we have input A B C that leads to an SLL conflict with full context X. That
 * implies that using X we might only use A B but we could also use A B C D to
 * resolve conflict. Input A B C D could predict alternative 1 in one position
 * in the input and A B C E could predict alternative 2 in another position in
 * input. The conflicting SLL configurations could still be non-unique in the
 * full context prediction, which would lead us to requiring more input than the
 * original A B C.	To make a	prediction cache work, we have to track	the exact
 * input	used during the previous prediction. That amounts to a cache that maps
 * X to a specific DFA for that context.</p>
 *
 * <p>
 * Something should be done for left-recursive expression predictions. They are
 * likely LL(1) + pred eval. Easier to do the whole SLL unless error and retry
 * with full LL thing Sam does.</p>
 *
 * <p>
 * <strong>AVOIDING FULL CONTEXT PREDICTION</strong></p>
 *
 * <p>
 * We avoid doing full context retry when the outer context is empty, we did not
 * dip into the outer context by falling off the end of the decision state rule,
 * or when we force SLL mode.</p>
 *
 * <p>
 * As an example of the not dip into outer context case, consider as super
 * constructor calls versus function calls. One grammar might look like
 * this:</p>
 *
 * <pre>
 * ctorBody
 *   : '{' superCall? stat* '}'
 *   ;
 * </pre>
 *
 * <p>
 * Or, you might see something like</p>
 *
 * <pre>
 * stat
 *   : superCall ';'
 *   | expression ';'
 *   | ...
 *   ;
 * </pre>
 *
 * <p>
 * In both cases I believe that no closure operations will dip into the outer
 * context. In the first case ctorBody in the worst case will stop at the '}'.
 * In the 2nd case it should stop at the ';'. Both cases should stay within the
 * entry rule and not dip into the outer context.</p>
 *
 * <p>
 * <strong>PREDICATES</strong></p>
 *
 * <p>
 * Predicates are always evaluated if present in either SLL or LL both. SLL and
 * LL simulation deals with predicates differently. SLL collects predicates as
 * it performs closure operations like ANTLR v3 did. It delays predicate
 * evaluation until it reaches and accept state. This allows us to cache the SLL
 * ATN simulation whereas, if we had evaluated predicates on-the-fly during
 * closure, the DFA state configuration sets would be different and we couldn't
 * build up a suitable DFA.</p>
 *
 * <p>
 * When building a DFA accept state during ATN simulation, we evaluate any
 * predicates and return the sole semantically valid alternative. If there is
 * more than 1 alternative, we report an ambiguity. If there are 0 alternatives,
 * we throw an exception. Alternatives without predicates act like they have
 * true predicates. The simple way to think about it is to strip away all
 * alternatives with false predicates and choose the minimum alternative that
 * remains.</p>
 *
 * <p>
 * When we start in the DFA and reach an accept state that's predicated, we test
 * those and return the minimum semantically viable alternative. If no
 * alternatives are viable, we throw an exception.</p>
 *
 * <p>
 * During full LL ATN simulation, closure always evaluates predicates and
 * on-the-fly. This is crucial to reducing the configuration set size during
 * closure. It hits a landmine when parsing with the Java grammar, for example,
 * without this on-the-fly evaluation.</p>
 *
 * <p>
 * <strong>SHARING DFA</strong></p>
 *
 * <p>
 * All instances of the same parser share the same decision DFAs through a
 * static field. Each instance gets its own ATN simulator but they share the
 * same {@link //decisionToDFA} field. They also share a
 * {@link PredictionContextCache} object that makes sure that all
 * {@link PredictionContext} objects are shared among the DFA states. This makes
 * a big size difference.</p>
 *
 * <p>
 * <strong>THREAD SAFETY</strong></p>
 *
 * <p>
 * The {@link ParserATNSimulator} locks on the {@link //decisionToDFA} field when
 * it adds a new DFA object to that array. {@link //addDFAEdge}
 * locks on the DFA for the current decision when setting the
 * {@link DFAState//edges} field. {@link //addDFAState} locks on
 * the DFA for the current decision when looking up a DFA state to see if it
 * already exists. We must make sure that all requests to add DFA states that
 * are equivalent result in the same shared DFA object. This is because lots of
 * threads will be trying to update the DFA at once. The
 * {@link //addDFAState} method also locks inside the DFA lock
 * but this time on the shared context cache when it rebuilds the
 * configurations' {@link PredictionContext} objects using cached
 * subgraphs/nodes. No other locking occurs, even during DFA simulation. This is
 * safe as long as we can guarantee that all threads referencing
 * {@code s.edge[t]} get the same physical target {@link DFAState}, or
 * {@code null}. Once into the DFA, the DFA simulation does not reference the
 * {@link DFA//states} map. It follows the {@link DFAState//edges} field to new
 * targets. The DFA simulator will either find {@link DFAState//edges} to be
 * {@code null}, to be non-{@code null} and {@code dfa.edges[t]} null, or
 * {@code dfa.edges[t]} to be non-null. The
 * {@link //addDFAEdge} method could be racing to set the field
 * but in either case the DFA simulator works; if {@code null}, and requests ATN
 * simulation. It could also race trying to get {@code dfa.edges[t]}, but either
 * way it will work because it's not doing a test and set operation.</p>
 *
 * <p>
 * <strong>Starting with SLL then failing to combined SLL/LL (Two-Stage
 * Parsing)</strong></p>
 *
 * <p>
 * Sam pointed out that if SLL does not give a syntax error, then there is no
 * point in doing full LL, which is slower. We only have to try LL if we get a
 * syntax error. For maximum speed, Sam starts the parser set to pure SLL
 * mode with the {@link BailErrorStrategy}:</p>
 *
 * <pre>
 * parser.{@link Parser//getInterpreter() getInterpreter()}.{@link //setPredictionMode setPredictionMode}{@code (}{@link PredictionMode//SLL}{@code )};
 * parser.{@link Parser//setErrorHandler setErrorHandler}(new {@link BailErrorStrategy}());
 * </pre>
 *
 * <p>
 * If it does not get a syntax error, then we're done. If it does get a syntax
 * error, we need to retry with the combined SLL/LL strategy.</p>
 *
 * <p>
 * The reason this works is as follows. If there are no SLL conflicts, then the
 * grammar is SLL (at least for that input set). If there is an SLL conflict,
 * the full LL analysis must yield a set of viable alternatives which is a
 * subset of the alternatives reported by SLL. If the LL set is a singleton,
 * then the grammar is LL but not SLL. If the LL set is the same size as the SLL
 * set, the decision is SLL. If the LL set has size &gt; 1, then that decision
 * is truly ambiguous on the current input. If the LL set is smaller, then the
 * SLL conflict resolution might choose an alternative that the full LL would
 * rule out as a possibility based upon better context information. If that's
 * the case, then the SLL parse will definitely get an error because the full LL
 * analysis says it's not viable. If SLL conflict resolution chooses an
 * alternative within the LL set, them both SLL and LL would choose the same
 * alternative because they both choose the minimum of multiple conflicting
 * alternatives.</p>
 *
 * <p>
 * Let's say we have a set of SLL conflicting alternatives {@code {1, 2, 3}} and
 * a smaller LL set called <em>s</em>. If <em>s</em> is {@code {2, 3}}, then SLL
 * parsing will get an error because SLL will pursue alternative 1. If
 * <em>s</em> is {@code {1, 2}} or {@code {1, 3}} then both SLL and LL will
 * choose the same alternative because alternative one is the minimum of either
 * set. If <em>s</em> is {@code {2}} or {@code {3}} then SLL will get a syntax
 * error. If <em>s</em> is {@code {1}} then SLL will succeed.</p>
 *
 * <p>
 * Of course, if the input is invalid, then we will get an error for sure in
 * both SLL and LL parsing. Erroneous input will therefore require 2 passes over
 * the input.</p>
 */
export class ParserATNSimulator extends ParserATNSimulator_base {
    constructor(parser: any, atn: any, decisionToDFA: any, sharedContextCache: any);
    parser: any;
    decisionToDFA: any;
    predictionMode: number;
    _input: any;
    _startIndex: number;
    _outerContext: any;
    _dfa: any;
    /**
     * Each prediction operation uses a cache for merge of prediction contexts.
     *  Don't keep around as it wastes huge amounts of memory. DoubleKeyMap
     *  isn't synchronized but we're ok since two threads shouldn't reuse same
     *  parser/atnsim object because it can only handle one input at a time.
     *  This maps graphs a and b to merged result c. (a,b)&rarr;c. We can avoid
     *  the merge if we ever see a and b again.  Note that (b,a)&rarr;c should
     *  also be examined during cache lookup.
     */
    mergeCache: {
        defaultMapCtor: any;
        cacheMap: any;
        get(a: any, b: any): any;
        set(a: any, b: any, o: any): void;
    } | null;
    debug: boolean;
    debug_closure: boolean;
    debug_add: boolean;
    trace_atn_sim: boolean;
    dfa_debug: boolean;
    retry_debug: boolean;
    reset(): void;
    adaptivePredict(input: any, decision: any, outerContext: any): any;
    /**
     * Performs ATN simulation to compute a predicted alternative based
     *  upon the remaining input, but also updates the DFA cache to avoid
     *  having to traverse the ATN again for the same input sequence.
     *
     * There are some key conditions we're looking for after computing a new
     * set of ATN configs (proposed DFA state):
     *       if the set is empty, there is no viable alternative for current symbol
     *       does the state uniquely predict an alternative?
     *       does the state have a conflict that would prevent us from
     *         putting it on the work list?
     *
     * We also have some key operations to do:
     *       add an edge from previous DFA state to potentially new DFA state, D,
     *         upon current symbol but only if adding to work list, which means in all
     *         cases except no viable alternative (and possibly non-greedy decisions?)
     *       collecting predicates and adding semantic context to DFA accept states
     *       adding rule context to context-sensitive DFA accept states
     *       consuming an input symbol
     *       reporting a conflict
     *       reporting an ambiguity
     *       reporting a context sensitivity
     *       reporting insufficient predicates
     *
     * cover these cases:
     *    dead end
     *    single alt
     *    single alt + preds
     *    conflict
     *    conflict + preds
     *
     */
    execATN(dfa: any, s0: any, input: any, startIndex: any, outerContext: any): any;
    /**
     * Get an existing target state for an edge in the DFA. If the target state
     * for the edge has not yet been computed or is otherwise not available,
     * this method returns {@code null}.
     *
     * @param previousD The current DFA state
     * @param t The next input symbol
     * @return The existing target DFA state for the given input symbol
     * {@code t}, or {@code null} if the target state for this edge is not
     * already cached
     */
    getExistingTargetState(previousD: any, t: any): any;
    /**
     * Compute a target state for an edge in the DFA, and attempt to add the
     * computed state and corresponding edge to the DFA.
     *
     * @param dfa The DFA
     * @param previousD The current DFA state
     * @param t The next input symbol
     *
     * @return The computed target DFA state for the given input symbol
     * {@code t}. If {@code t} does not lead to a valid DFA state, this method
     * returns {@link //ERROR
     */
    computeTargetState(dfa: any, previousD: any, t: any): {
        stateNumber: any;
        configs: any;
        /**
         * {@code edges[symbol]} points to target of symbol. Shift up by 1 so (-1)
         * {@link Token//EOF} maps to {@code edges[0]}.
         */
        edges: any;
        isAcceptState: boolean;
        /**
         * if accept state, what ttype do we match or alt do we predict?
         * This is set to {@link ATN//INVALID_ALT_NUMBER} when {@link//predicates}
         * {@code !=null} or {@link //requiresFullContext}.
         */
        prediction: number;
        lexerActionExecutor: any;
        /**
         * Indicates that this state was created during SLL prediction that
         * discovered a conflict between the configurations in the state. Future
         * {@link ParserATNSimulator//execATN} invocations immediately jumped doing
         * full context prediction if this field is true.
         */
        requiresFullContext: boolean;
        /**
         * During SLL parsing, this is a list of predicates associated with the
         * ATN configurations of the DFA state. When we have predicates,
         * {@link //requiresFullContext} is {@code false} since full context
         * prediction evaluates predicates
         * on-the-fly. If this is not null, then {@link //prediction} is
         * {@link ATN//INVALID_ALT_NUMBER}.
         *
         * <p>We only use these for non-{@link //requiresFullContext} but
         * conflicting states. That
         * means we know from the context (it's $ or we don't dip into outer
         * context) that it's an ambiguity not a conflict.</p>
         *
         * <p>This list is computed by {@link
         * ParserATNSimulator//predicateDFAState}.</p>
         */
        predicates: any;
        /**
         * Get the set of all alts mentioned by all ATN configurations in this
         * DFA state.
         */
        getAltSet(): {
            data: {};
            hashFunction: any;
            equalsFunction: any;
            add(value: any): any;
            has(value: any): boolean;
            get(value: any): any;
            values(): any[];
            toString(): string;
            readonly length: any;
        } | null;
        /**
         * Two {@link DFAState} instances are equal if their ATN configuration sets
         * are the same. This method is used to see if a state already exists.
         *
         * <p>Because the number of alternatives and number of ATN configurations are
         * finite, there is a finite number of DFA states that can be processed.
         * This is necessary to show that the algorithm terminates.</p>
         *
         * <p>Cannot test the DFA state numbers here because in
         * {@link ParserATNSimulator//addDFAState} we need to know if any other state
         * exists that has this exact set of ATN configurations. The
         * {@link //stateNumber} is irrelevant.</p>
         */
        equals(other: any): any;
        toString(): string;
        hashCode(): number;
    };
    predicateDFAState(dfaState: any, decisionState: any): void;
    execATNWithFullContext(dfa: any, D: any, s0: any, input: any, startIndex: any, outerContext: any): number;
    computeReachSet(closure: any, t: any, fullCtx: any): any;
    /**
     * Return a configuration set containing only the configurations from
     * {@code configs} which are in a {@link RuleStopState}. If all
     * configurations in {@code configs} are already in a rule stop state, this
     * method simply returns {@code configs}.
     *
     * <p>When {@code lookToEndOfRule} is true, this method uses
     * {@link ATN//nextTokens} for each configuration in {@code configs} which is
     * not already in a rule stop state to see if a rule stop state is reachable
     * from the configuration via epsilon-only transitions.</p>
     *
     * @param configs the configuration set to update
     * @param lookToEndOfRule when true, this method checks for rule stop states
     * reachable by epsilon-only transitions from each configuration in
     * {@code configs}.
     *
     * @return {@code configs} if all configurations in {@code configs} are in a
     * rule stop state, otherwise return a new configuration set containing only
     * the configurations from {@code configs} which are in a rule stop state
     */
    removeAllConfigsNotInRuleStopState(configs: any, lookToEndOfRule: any): any;
    computeStartState(p: any, ctx: any, fullCtx: any): {
        /**
         * The reason that we need this is because we don't want the hash map to use
         * the standard hash code and equals. We need all configurations with the
         * same
         * {@code (s,i,_,semctx)} to be equal. Unfortunately, this key effectively
         * doubles
         * the number of objects associated with ATNConfigs. The other solution is
         * to
         * use a hash table that lets us specify the equals/hashcode operation.
         * All configs but hashed by (s, i, _, pi) not including context. Wiped out
         * when we go readonly as this set becomes a DFA state
         */
        configLookup: {
            data: {};
            hashFunction: any;
            equalsFunction: any;
            add(value: any): any;
            has(value: any): boolean;
            get(value: any): any;
            values(): any[];
            toString(): string;
            readonly length: any;
        };
        /**
         * Indicates that this configuration set is part of a full context
         * LL prediction. It will be used to determine how to merge $. With SLL
         * it's a wildcard whereas it is not for LL context merge
         */
        fullCtx: any;
        /**
         * Indicates that the set of configurations is read-only. Do not
         * allow any code to manipulate the set; DFA states will point at
         * the sets and they must not change. This does not protect the other
         * fields; in particular, conflictingAlts is set after
         * we've made this readonly
         */
        readOnly: boolean;
        configs: any[];
        uniqueAlt: number;
        conflictingAlts: any;
        /**
         * Used in parser and lexer. In lexer, it indicates we hit a pred
         * while computing a closure operation. Don't make a DFA state from this
         */
        hasSemanticContext: boolean;
        dipsIntoOuterContext: boolean;
        cachedHashCode: number;
        /**
         * Adding a new config means merging contexts with existing configs for
         * {@code (s, i, pi, _)}, where {@code s} is the
         * {@link ATNConfig//state}, {@code i} is the {@link ATNConfig//alt}, and
         * {@code pi} is the {@link ATNConfig//semanticContext}. We use
         * {@code (s,i,pi)} as key.
         *
         * <p>This method updates {@link //dipsIntoOuterContext} and
         * {@link //hasSemanticContext} when necessary.</p>
         */
        add(config: any, mergeCache: any): boolean;
        getStates(): {
            data: {};
            hashFunction: any;
            equalsFunction: any;
            add(value: any): any;
            has(value: any): boolean;
            get(value: any): any;
            values(): any[];
            toString(): string;
            readonly length: any;
        };
        getPredicates(): any[];
        optimizeConfigs(interpreter: any): void;
        addAll(coll: any): boolean;
        equals(other: any): boolean;
        hashCode(): number;
        updateHashCode(hash: any): void;
        isEmpty(): boolean;
        contains(item: any): any;
        containsFast(item: any): any;
        clear(): void;
        setReadonly(readOnly: any): void;
        toString(): string;
        readonly items: any[];
        readonly length: number;
    };
    /**
     * This method transforms the start state computed by
     * {@link //computeStartState} to the special start state used by a
     * precedence DFA for a particular precedence value. The transformation
     * process applies the following changes to the start state's configuration
     * set.
     *
     * <ol>
     * <li>Evaluate the precedence predicates for each configuration using
     * {@link SemanticContext//evalPrecedence}.</li>
     * <li>Remove all configurations which predict an alternative greater than
     * 1, for which another configuration that predicts alternative 1 is in the
     * same ATN state with the same prediction context. This transformation is
     * valid for the following reasons:
     * <ul>
     * <li>The closure block cannot contain any epsilon transitions which bypass
     * the body of the closure, so all states reachable via alternative 1 are
     * part of the precedence alternatives of the transformed left-recursive
     * rule.</li>
     * <li>The "primary" portion of a left recursive rule cannot contain an
     * epsilon transition, so the only way an alternative other than 1 can exist
     * in a state that is also reachable via alternative 1 is by nesting calls
     * to the left-recursive rule, with the outer calls not being at the
     * preferred precedence level.</li>
     * </ul>
     * </li>
     * </ol>
     *
     * <p>
     * The prediction context must be considered by this filter to address
     * situations like the following.
     * </p>
     * <code>
     * <pre>
     * grammar TA;
     * prog: statement* EOF;
     * statement: letterA | statement letterA 'b' ;
     * letterA: 'a';
     * </pre>
     * </code>
     * <p>
     * If the above grammar, the ATN state immediately before the token
     * reference {@code 'a'} in {@code letterA} is reachable from the left edge
     * of both the primary and closure blocks of the left-recursive rule
     * {@code statement}. The prediction context associated with each of these
     * configurations distinguishes between them, and prevents the alternative
     * which stepped out to {@code prog} (and then back in to {@code statement}
     * from being eliminated by the filter.
     * </p>
     *
     * @param configs The configuration set computed by
     * {@link //computeStartState} as the start state for the DFA.
     * @return The transformed configuration set representing the start state
     * for a precedence DFA at a particular precedence level (determined by
     * calling {@link Parser//getPrecedence})
     */
    applyPrecedenceFilter(configs: any): {
        /**
         * The reason that we need this is because we don't want the hash map to use
         * the standard hash code and equals. We need all configurations with the
         * same
         * {@code (s,i,_,semctx)} to be equal. Unfortunately, this key effectively
         * doubles
         * the number of objects associated with ATNConfigs. The other solution is
         * to
         * use a hash table that lets us specify the equals/hashcode operation.
         * All configs but hashed by (s, i, _, pi) not including context. Wiped out
         * when we go readonly as this set becomes a DFA state
         */
        configLookup: {
            data: {};
            hashFunction: any;
            equalsFunction: any;
            add(value: any): any;
            has(value: any): boolean;
            get(value: any): any;
            values(): any[];
            toString(): string;
            readonly length: any;
        };
        /**
         * Indicates that this configuration set is part of a full context
         * LL prediction. It will be used to determine how to merge $. With SLL
         * it's a wildcard whereas it is not for LL context merge
         */
        fullCtx: any;
        /**
         * Indicates that the set of configurations is read-only. Do not
         * allow any code to manipulate the set; DFA states will point at
         * the sets and they must not change. This does not protect the other
         * fields; in particular, conflictingAlts is set after
         * we've made this readonly
         */
        readOnly: boolean;
        configs: any[];
        uniqueAlt: number;
        conflictingAlts: any;
        /**
         * Used in parser and lexer. In lexer, it indicates we hit a pred
         * while computing a closure operation. Don't make a DFA state from this
         */
        hasSemanticContext: boolean;
        dipsIntoOuterContext: boolean;
        cachedHashCode: number;
        /**
         * Adding a new config means merging contexts with existing configs for
         * {@code (s, i, pi, _)}, where {@code s} is the
         * {@link ATNConfig//state}, {@code i} is the {@link ATNConfig//alt}, and
         * {@code pi} is the {@link ATNConfig//semanticContext}. We use
         * {@code (s,i,pi)} as key.
         *
         * <p>This method updates {@link //dipsIntoOuterContext} and
         * {@link //hasSemanticContext} when necessary.</p>
         */
        add(config: any, mergeCache: any): boolean;
        getStates(): {
            data: {};
            hashFunction: any;
            equalsFunction: any;
            add(value: any): any;
            has(value: any): boolean;
            get(value: any): any;
            values(): any[];
            toString(): string;
            readonly length: any;
        };
        getPredicates(): any[];
        optimizeConfigs(interpreter: any): void;
        addAll(coll: any): boolean;
        equals(other: any): boolean;
        hashCode(): number;
        updateHashCode(hash: any): void;
        isEmpty(): boolean;
        contains(item: any): any;
        containsFast(item: any): any;
        clear(): void;
        setReadonly(readOnly: any): void;
        toString(): string;
        readonly items: any[];
        readonly length: number;
    };
    getReachableTarget(trans: any, ttype: any): any;
    getPredsForAmbigAlts(ambigAlts: any, configs: any, nalts: any): any[];
    getPredicatePredictions(ambigAlts: any, altToPred: any): {
        alt: any;
        pred: any;
        toString(): string;
    }[] | null;
    /**
     * This method is used to improve the localization of error messages by
     * choosing an alternative rather than throwing a
     * {@link NoViableAltException} in particular prediction scenarios where the
     * {@link //ERROR} state was reached during ATN simulation.
     *
     * <p>
     * The default implementation of this method uses the following
     * algorithm to identify an ATN configuration which successfully parsed the
     * decision entry rule. Choosing such an alternative ensures that the
     * {@link ParserRuleContext} returned by the calling rule will be complete
     * and valid, and the syntax error will be reported later at a more
     * localized location.</p>
     *
     * <ul>
     * <li>If a syntactically valid path or paths reach the end of the decision rule and
     * they are semantically valid if predicated, return the min associated alt.</li>
     * <li>Else, if a semantically invalid but syntactically valid path exist
     * or paths exist, return the minimum associated alt.
     * </li>
     * <li>Otherwise, return {@link ATN//INVALID_ALT_NUMBER}.</li>
     * </ul>
     *
     * <p>
     * In some scenarios, the algorithm described above could predict an
     * alternative which will result in a {@link FailedPredicateException} in
     * the parser. Specifically, this could occur if the <em>only</em> configuration
     * capable of successfully parsing to the end of the decision rule is
     * blocked by a semantic predicate. By choosing this alternative within
     * {@link //adaptivePredict} instead of throwing a
     * {@link NoViableAltException}, the resulting
     * {@link FailedPredicateException} in the parser will identify the specific
     * predicate which is preventing the parser from successfully parsing the
     * decision rule, which helps developers identify and correct logic errors
     * in semantic predicates.
     * </p>
     *
     * @param configs The ATN configurations which were valid immediately before
     * the {@link //ERROR} state was reached
     * @param outerContext The is the \gamma_0 initial parser context from the paper
     * or the parser stack at the instant before prediction commences.
     *
     * @return The value to return from {@link //adaptivePredict}, or
     * {@link ATN//INVALID_ALT_NUMBER} if a suitable alternative was not
     * identified and {@link //adaptivePredict} should report an error instead
     */
    getSynValidOrSemInvalidAltThatFinishedDecisionEntryRule(configs: any, outerContext: any): number;
    getAltThatFinishedDecisionEntryRule(configs: any): number;
    /**
     * Walk the list of configurations and split them according to
     * those that have preds evaluating to true/false.  If no pred, assume
     * true pred and include in succeeded set.  Returns Pair of sets.
     *
     * Create a new set so as not to alter the incoming parameter.
     *
     * Assumption: the input stream has been restored to the starting point
     * prediction, which is where predicates need to evaluate.*/
    splitAccordingToSemanticValidity(configs: any, outerContext: any): {
        /**
         * The reason that we need this is because we don't want the hash map to use
         * the standard hash code and equals. We need all configurations with the
         * same
         * {@code (s,i,_,semctx)} to be equal. Unfortunately, this key effectively
         * doubles
         * the number of objects associated with ATNConfigs. The other solution is
         * to
         * use a hash table that lets us specify the equals/hashcode operation.
         * All configs but hashed by (s, i, _, pi) not including context. Wiped out
         * when we go readonly as this set becomes a DFA state
         */
        configLookup: {
            data: {};
            hashFunction: any;
            equalsFunction: any;
            add(value: any): any;
            has(value: any): boolean;
            get(value: any): any;
            values(): any[];
            toString(): string;
            readonly length: any;
        };
        /**
         * Indicates that this configuration set is part of a full context
         * LL prediction. It will be used to determine how to merge $. With SLL
         * it's a wildcard whereas it is not for LL context merge
         */
        fullCtx: any;
        /**
         * Indicates that the set of configurations is read-only. Do not
         * allow any code to manipulate the set; DFA states will point at
         * the sets and they must not change. This does not protect the other
         * fields; in particular, conflictingAlts is set after
         * we've made this readonly
         */
        readOnly: boolean;
        configs: any[];
        uniqueAlt: number;
        conflictingAlts: any;
        /**
         * Used in parser and lexer. In lexer, it indicates we hit a pred
         * while computing a closure operation. Don't make a DFA state from this
         */
        hasSemanticContext: boolean;
        dipsIntoOuterContext: boolean;
        cachedHashCode: number;
        /**
         * Adding a new config means merging contexts with existing configs for
         * {@code (s, i, pi, _)}, where {@code s} is the
         * {@link ATNConfig//state}, {@code i} is the {@link ATNConfig//alt}, and
         * {@code pi} is the {@link ATNConfig//semanticContext}. We use
         * {@code (s,i,pi)} as key.
         *
         * <p>This method updates {@link //dipsIntoOuterContext} and
         * {@link //hasSemanticContext} when necessary.</p>
         */
        add(config: any, mergeCache: any): boolean;
        getStates(): {
            data: {};
            hashFunction: any;
            equalsFunction: any;
            add(value: any): any;
            has(value: any): boolean;
            get(value: any): any;
            values(): any[];
            toString(): string;
            readonly length: any;
        };
        getPredicates(): any[];
        optimizeConfigs(interpreter: any): void;
        addAll(coll: any): boolean;
        equals(other: any): boolean;
        hashCode(): number;
        updateHashCode(hash: any): void;
        isEmpty(): boolean;
        contains(item: any): any;
        containsFast(item: any): any;
        clear(): void;
        setReadonly(readOnly: any): void;
        toString(): string;
        readonly items: any[];
        readonly length: number;
    }[];
    /**
     * Look through a list of predicate/alt pairs, returning alts for the
     * pairs that win. A {@code NONE} predicate indicates an alt containing an
     * unpredicated config which behaves as "always true." If !complete
     * then we stop at the first predicate that evaluates to true. This
     * includes pairs with null predicates.
     */
    evalSemanticContext(predPredictions: any, outerContext: any, complete: any): {
        data: any[];
        add(value: any): void;
        or(set: any): void;
        remove(value: any): void;
        has(value: any): boolean;
        values(): string[];
        minValue(): number;
        hashCode(): number;
        equals(other: any): boolean;
        toString(): string;
        readonly length: number;
    };
    closure(config: any, configs: any, closureBusy: any, collectPredicates: any, fullCtx: any, treatEofAsEpsilon: any): void;
    closureCheckingStopState(config: any, configs: any, closureBusy: any, collectPredicates: any, fullCtx: any, depth: any, treatEofAsEpsilon: any): void;
    closure_(config: any, configs: any, closureBusy: any, collectPredicates: any, fullCtx: any, depth: any, treatEofAsEpsilon: any): void;
    canDropLoopEntryEdgeInLeftRecursiveRule(config: any): boolean;
    getRuleName(index: any): any;
    getEpsilonTarget(config: any, t: any, collectPredicates: any, inContext: any, fullCtx: any, treatEofAsEpsilon: any): {
        state: any;
        alt: any;
        /**
         * The stack of invoking states leading to the rule/states associated
         * with this config.  We track only those contexts pushed during
         * execution of the ATN simulator
         */
        context: any;
        semanticContext: any;
        /**
         * We cannot execute predicates dependent upon local context unless
         * we know for sure we are in the correct context. Because there is
         * no way to do this efficiently, we simply cannot evaluate
         * dependent predicates unless we are in the rule that initially
         * invokes the ATN simulator.
         * closure() tracks the depth of how far we dip into the
         * outer context: depth &gt; 0.  Note that it may not be totally
         * accurate depth since I don't ever decrement
         */
        reachesIntoOuterContext: any;
        precedenceFilterSuppressed: any;
        checkContext(params: any, config: any): void;
        hashCode(): number;
        updateHashCode(hash: any): void;
        /**
         * An ATN configuration is equal to another if both have
         * the same state, they predict the same alternative, and
         * syntactic/semantic contexts are the same
         */
        equals(other: any): any;
        hashCodeForConfigSet(): number;
        equalsForConfigSet(other: any): any;
        toString(): string;
    } | null;
    actionTransition(config: any, t: any): {
        state: any;
        alt: any;
        /**
         * The stack of invoking states leading to the rule/states associated
         * with this config.  We track only those contexts pushed during
         * execution of the ATN simulator
         */
        context: any;
        semanticContext: any;
        /**
         * We cannot execute predicates dependent upon local context unless
         * we know for sure we are in the correct context. Because there is
         * no way to do this efficiently, we simply cannot evaluate
         * dependent predicates unless we are in the rule that initially
         * invokes the ATN simulator.
         * closure() tracks the depth of how far we dip into the
         * outer context: depth &gt; 0.  Note that it may not be totally
         * accurate depth since I don't ever decrement
         */
        reachesIntoOuterContext: any;
        precedenceFilterSuppressed: any;
        checkContext(params: any, config: any): void;
        hashCode(): number;
        updateHashCode(hash: any): void;
        /**
         * An ATN configuration is equal to another if both have
         * the same state, they predict the same alternative, and
         * syntactic/semantic contexts are the same
         */
        equals(other: any): any;
        hashCodeForConfigSet(): number;
        equalsForConfigSet(other: any): any;
        toString(): string;
    };
    precedenceTransition(config: any, pt: any, collectPredicates: any, inContext: any, fullCtx: any): {
        state: any;
        alt: any;
        /**
         * The stack of invoking states leading to the rule/states associated
         * with this config.  We track only those contexts pushed during
         * execution of the ATN simulator
         */
        context: any;
        semanticContext: any;
        /**
         * We cannot execute predicates dependent upon local context unless
         * we know for sure we are in the correct context. Because there is
         * no way to do this efficiently, we simply cannot evaluate
         * dependent predicates unless we are in the rule that initially
         * invokes the ATN simulator.
         * closure() tracks the depth of how far we dip into the
         * outer context: depth &gt; 0.  Note that it may not be totally
         * accurate depth since I don't ever decrement
         */
        reachesIntoOuterContext: any;
        precedenceFilterSuppressed: any;
        checkContext(params: any, config: any): void;
        hashCode(): number;
        updateHashCode(hash: any): void;
        /**
         * An ATN configuration is equal to another if both have
         * the same state, they predict the same alternative, and
         * syntactic/semantic contexts are the same
         */
        equals(other: any): any;
        hashCodeForConfigSet(): number;
        equalsForConfigSet(other: any): any;
        toString(): string;
    } | null;
    predTransition(config: any, pt: any, collectPredicates: any, inContext: any, fullCtx: any): {
        state: any;
        alt: any;
        /**
         * The stack of invoking states leading to the rule/states associated
         * with this config.  We track only those contexts pushed during
         * execution of the ATN simulator
         */
        context: any;
        semanticContext: any;
        /**
         * We cannot execute predicates dependent upon local context unless
         * we know for sure we are in the correct context. Because there is
         * no way to do this efficiently, we simply cannot evaluate
         * dependent predicates unless we are in the rule that initially
         * invokes the ATN simulator.
         * closure() tracks the depth of how far we dip into the
         * outer context: depth &gt; 0.  Note that it may not be totally
         * accurate depth since I don't ever decrement
         */
        reachesIntoOuterContext: any;
        precedenceFilterSuppressed: any;
        checkContext(params: any, config: any): void;
        hashCode(): number;
        updateHashCode(hash: any): void;
        /**
         * An ATN configuration is equal to another if both have
         * the same state, they predict the same alternative, and
         * syntactic/semantic contexts are the same
         */
        equals(other: any): any;
        hashCodeForConfigSet(): number;
        equalsForConfigSet(other: any): any;
        toString(): string;
    } | null;
    ruleTransition(config: any, t: any): {
        state: any;
        alt: any;
        /**
         * The stack of invoking states leading to the rule/states associated
         * with this config.  We track only those contexts pushed during
         * execution of the ATN simulator
         */
        context: any;
        semanticContext: any;
        /**
         * We cannot execute predicates dependent upon local context unless
         * we know for sure we are in the correct context. Because there is
         * no way to do this efficiently, we simply cannot evaluate
         * dependent predicates unless we are in the rule that initially
         * invokes the ATN simulator.
         * closure() tracks the depth of how far we dip into the
         * outer context: depth &gt; 0.  Note that it may not be totally
         * accurate depth since I don't ever decrement
         */
        reachesIntoOuterContext: any;
        precedenceFilterSuppressed: any;
        checkContext(params: any, config: any): void;
        hashCode(): number;
        updateHashCode(hash: any): void;
        /**
         * An ATN configuration is equal to another if both have
         * the same state, they predict the same alternative, and
         * syntactic/semantic contexts are the same
         */
        equals(other: any): any;
        hashCodeForConfigSet(): number;
        equalsForConfigSet(other: any): any;
        toString(): string;
    };
    getConflictingAlts(configs: any): {
        data: any[];
        add(value: any): void;
        or(set: any): void;
        remove(value: any): void;
        has(value: any): boolean;
        values(): string[];
        minValue(): number;
        hashCode(): number;
        equals(other: any): boolean;
        toString(): string;
        readonly length: number;
    };
    /**
     * Sam pointed out a problem with the previous definition, v3, of
     * ambiguous states. If we have another state associated with conflicting
     * alternatives, we should keep going. For example, the following grammar
     *
     * s : (ID | ID ID?) ';' ;
     *
     * When the ATN simulation reaches the state before ';', it has a DFA
     * state that looks like: [12|1|[], 6|2|[], 12|2|[]]. Naturally
     * 12|1|[] and 12|2|[] conflict, but we cannot stop processing this node
     * because alternative to has another way to continue, via [6|2|[]].
     * The key is that we have a single state that has config's only associated
     * with a single alternative, 2, and crucially the state transitions
     * among the configurations are all non-epsilon transitions. That means
     * we don't consider any conflicts that include alternative 2. So, we
     * ignore the conflict between alts 1 and 2. We ignore a set of
     * conflicting alts when there is an intersection with an alternative
     * associated with a single alt state in the state&rarr;config-list map.
     *
     * It's also the case that we might have two conflicting configurations but
     * also a 3rd nonconflicting configuration for a different alternative:
     * [1|1|[], 1|2|[], 8|3|[]]. This can come about from grammar:
     *
     * a : A | A | A B ;
     *
     * After matching input A, we reach the stop state for rule A, state 1.
     * State 8 is the state right before B. Clearly alternatives 1 and 2
     * conflict and no amount of further lookahead will separate the two.
     * However, alternative 3 will be able to continue and so we do not
     * stop working on this state. In the previous example, we're concerned
     * with states associated with the conflicting alternatives. Here alt
     * 3 is not associated with the conflicting configs, but since we can continue
     * looking for input reasonably, I don't declare the state done. We
     * ignore a set of conflicting alts when we have an alternative
     * that we still need to pursue
     */
    getConflictingAltsOrUniqueAlt(configs: any): any;
    getTokenName(t: any): string;
    getLookaheadName(input: any): string;
    /**
     * Used for debugging in adaptivePredict around execATN but I cut
     * it out for clarity now that alg. works well. We can leave this
     * "dead" code for a bit
     */
    dumpDeadEndConfigs(nvae: any): void;
    noViableAlt(input: any, outerContext: any, configs: any, startIndex: any): {
        deadEndConfigs: any;
        startToken: any;
        offendingToken: any;
        message: any;
        recognizer: any;
        input: any;
        ctx: any;
        /**
         * Get the ATN state number the parser was in at the time the error
         * occurred. For {@link NoViableAltException} and
         * {@link LexerNoViableAltException} exceptions, this is the
         * {@link DecisionState} number. For others, it is the state whose outgoing
         * edge we couldn't match.
         */
        offendingState: any;
        /**
         * Gets the set of input symbols which could potentially follow the
         * previously matched symbol at the time this exception was thrown.
         *
         * <p>If the set of expected tokens is not known and could not be computed,
         * this method returns {@code null}.</p>
         *
         * @return The set of token types that could potentially follow the current
         * state in the ATN, or {@code null} if the information is not available.
         */
        getExpectedTokens(): any;
        toString(): any;
        name: string;
        stack?: string | undefined;
    };
    getUniqueAlt(configs: any): number;
    /**
     * Add an edge to the DFA, if possible. This method calls
     * {@link //addDFAState} to ensure the {@code to} state is present in the
     * DFA. If {@code from} is {@code null}, or if {@code t} is outside the
     * range of edges that can be represented in the DFA tables, this method
     * returns without adding the edge to the DFA.
     *
     * <p>If {@code to} is {@code null}, this method returns {@code null}.
     * Otherwise, this method returns the {@link DFAState} returned by calling
     * {@link //addDFAState} for the {@code to} state.</p>
     *
     * @param dfa The DFA
     * @param from_ The source state for the edge
     * @param t The input symbol
     * @param to The target state for the edge
     *
     * @return If {@code to} is {@code null}, this method returns {@code null};
     * otherwise this method returns the result of calling {@link //addDFAState}
     * on {@code to}
     */
    addDFAEdge(dfa: any, from_: any, t: any, to: any): any;
    /**
     * Add state {@code D} to the DFA if it is not already present, and return
     * the actual instance stored in the DFA. If a state equivalent to {@code D}
     * is already in the DFA, the existing state is returned. Otherwise this
     * method returns {@code D} after adding it to the DFA.
     *
     * <p>If {@code D} is {@link //ERROR}, this method returns {@link //ERROR} and
     * does not change the DFA.</p>
     *
     * @param dfa The dfa
     * @param D The DFA state to add
     * @return The state stored in the DFA. This will be either the existing
     * state if {@code D} is already in the DFA, or {@code D} itself if the
     * state was not already present
     */
    addDFAState(dfa: any, D: any): any;
    reportAttemptingFullContext(dfa: any, conflictingAlts: any, configs: any, startIndex: any, stopIndex: any): void;
    reportContextSensitivity(dfa: any, prediction: any, configs: any, startIndex: any, stopIndex: any): void;
    reportAmbiguity(dfa: any, D: any, startIndex: any, stopIndex: any, exact: any, ambigAlts: any, configs: any): void;
}
declare const ParserRuleContext_base: {
    new (parent: any, invokingState: any): {
        parentCtx: any;
        /**
         * What state invoked the rule associated with this context?
         * The "return address" is the followState of invokingState
         * If parent is null, this should be -1.
         */
        invokingState: any;
        depth(): number;
        /**
         * A context is empty if there is no invoking state; meaning nobody call
         * current context.
         */
        isEmpty(): boolean;
        getSourceInterval(): {
            start: any;
            stop: any;
            clone(): any;
            contains(item: any): boolean;
            toString(): any;
            readonly length: number;
        };
        readonly ruleContext: any;
        getPayload(): any;
        /**
         * Return the combined text of all child nodes. This method only considers
         * tokens which have been added to the parse tree.
         * <p>
         * Since tokens on hidden channels (e.g. whitespace or comments) are not
         * added to the parse trees, they will not appear in the output of this
         * method.
         */
        getText(): any;
        /**
         * For rule associated with this parse tree internal node, return
         * the outer alternative number used to match the input. Default
         * implementation does not compute nor store this alt num. Create
         * a subclass of ParserRuleContext with backing field and set
         * option contextSuperClass.
         * to set it.
         */
        getAltNumber(): number;
        /**
         * Set the outer alternative number for this context node. Default
         * implementation does nothing to avoid backing field overhead for
         * trees that don't need it.  Create
         * a subclass of ParserRuleContext with backing field and set
         * option contextSuperClass.
         */
        setAltNumber(altNumber: any): void;
        getChild(i: any): null;
        getChildCount(): number;
        accept(visitor: any): any;
        /**
         * Print out a whole tree, not just a node, in LISP format
         * (root child1 .. childN). Print just a node if this is a leaf.
         */
        toStringTree(ruleNames: any, recog: any): any;
        toString(ruleNames: any, stop: any): string;
    };
    EMPTY: {
        /**
         * If we are debugging or building a parse tree for a visitor,
         * we need to track all of the tokens and rule invocations associated
         * with this rule's context. This is empty for parsing w/o tree constr.
         * operation because we don't the need to track the details about
         * how we parse this rule.
         */
        children: any[] | null;
        start: any;
        stop: any;
        /**
         * The exception that forced this rule to return. If the rule successfully
         * completed, this is {@code null}.
         */
        exception: any;
        copyFrom(ctx: any): void;
        parentCtx: any;
        invokingState: any;
        enterRule(listener: any): void;
        exitRule(listener: any): void;
        addChild(child: any): any;
        /** Used by enterOuterAlt to toss out a RuleContext previously added as
         * we entered a rule. If we have // label, we will need to remove
         * generic ruleContext object.
         */
        removeLastChild(): void;
        addTokenNode(token: any): {
            parentCtx: any;
            symbol: any;
            getChild(i: any): null;
            getSymbol(): any;
            getParent(): any;
            getPayload(): any;
            getSourceInterval(): {
                start: any;
                stop: any;
                clone(): any;
                contains(item: any): boolean;
                toString(): any;
                readonly length: number;
            };
            getChildCount(): number;
            accept(visitor: any): any;
            getText(): any;
            toString(): any;
        };
        addErrorNode(badToken: any): {
            isErrorNode(): boolean;
            accept(visitor: any): any;
            parentCtx: any;
            symbol: any;
            getChild(i: any): null;
            getSymbol(): any;
            getParent(): any;
            getPayload(): any;
            getSourceInterval(): {
                start: any;
                stop: any;
                clone(): any;
                contains(item: any): boolean;
                toString(): any;
                readonly length: number;
            };
            getChildCount(): number;
            getText(): any;
            toString(): any;
        };
        getChild(i: any, type: any): any;
        getToken(ttype: any, i: any): {} | null;
        getTokens(ttype: any): {}[];
        getTypedRuleContext(ctxType: any, i: any): any;
        getTypedRuleContexts(ctxType: any): any[];
        getChildCount(): number;
        getSourceInterval(): {
            start: any;
            stop: any;
            clone(): any;
            contains(item: any): boolean;
            toString(): any;
            readonly length: number;
        };
        depth(): number;
        /**
         * A context is empty if there is no invoking state; meaning nobody call
         * current context.
         */
        isEmpty(): boolean;
        readonly ruleContext: any;
        getPayload(): any;
        /**
         * Return the combined text of all child nodes. This method only considers
         * tokens which have been added to the parse tree.
         * <p>
         * Since tokens on hidden channels (e.g. whitespace or comments) are not
         * added to the parse trees, they will not appear in the output of this
         * method.
         */
        getText(): any;
        /**
         * For rule associated with this parse tree internal node, return
         * the outer alternative number used to match the input. Default
         * implementation does not compute nor store this alt num. Create
         * a subclass of ParserRuleContext with backing field and set
         * option contextSuperClass.
         * to set it.
         */
        getAltNumber(): number;
        /**
         * Set the outer alternative number for this context node. Default
         * implementation does nothing to avoid backing field overhead for
         * trees that don't need it.  Create
         * a subclass of ParserRuleContext with backing field and set
         * option contextSuperClass.
         */
        setAltNumber(altNumber: any): void;
        accept(visitor: any): any;
        /**
         * Print out a whole tree, not just a node, in LISP format
         * (root child1 .. childN). Print just a node if this is a leaf.
         */
        toStringTree(ruleNames: any, recog: any): any;
        toString(ruleNames: any, stop: any): string;
    };
};
/**
 * A rule invocation record for parsing.
 *
 *  Contains all of the information about the current rule not stored in the
 *  RuleContext. It handles parse tree children list, Any ATN state
 *  tracing, and the default values available for rule indications:
 *  start, stop, rule index, current alt number, current
 *  ATN state.
 *
 *  Subclasses made for each rule and grammar track the parameters,
 *  return values, locals, and labels specific to that rule. These
 *  are the objects that are returned from rules.
 *
 *  Note text is not an actual field of a rule return value; it is computed
 *  from start and stop using the input stream's toString() method.  I
 *  could add a ctor to this so that we can pass in and store the input
 *  stream, but I'm not sure we want to do that.  It would seem to be undefined
 *  to get the .text property anyway if the rule matches tokens from multiple
 *  input streams.
 *
 *  I do not use getters for fields of objects that are used simply to
 *  group values such as this aggregate.  The getters/setters are there to
 *  satisfy the superclass interface.
 */
export class ParserRuleContext extends ParserRuleContext_base {
    /**
     * If we are debugging or building a parse tree for a visitor,
     * we need to track all of the tokens and rule invocations associated
     * with this rule's context. This is empty for parsing w/o tree constr.
     * operation because we don't the need to track the details about
     * how we parse this rule.
     */
    children: any[] | null;
    start: any;
    stop: any;
    /**
     * The exception that forced this rule to return. If the rule successfully
     * completed, this is {@code null}.
     */
    exception: any;
    copyFrom(ctx: any): void;
    enterRule(listener: any): void;
    exitRule(listener: any): void;
    addChild(child: any): any;
    /** Used by enterOuterAlt to toss out a RuleContext previously added as
     * we entered a rule. If we have // label, we will need to remove
     * generic ruleContext object.
     */
    removeLastChild(): void;
    addTokenNode(token: any): {
        parentCtx: any;
        symbol: any;
        getChild(i: any): null;
        getSymbol(): any;
        getParent(): any;
        getPayload(): any;
        getSourceInterval(): {
            start: any;
            stop: any;
            clone(): any;
            contains(item: any): boolean;
            toString(): any;
            readonly length: number;
        };
        getChildCount(): number;
        accept(visitor: any): any;
        getText(): any;
        toString(): any;
    };
    addErrorNode(badToken: any): {
        isErrorNode(): boolean;
        accept(visitor: any): any;
        parentCtx: any;
        symbol: any;
        getChild(i: any): null;
        getSymbol(): any;
        getParent(): any;
        getPayload(): any;
        getSourceInterval(): {
            start: any;
            stop: any;
            clone(): any;
            contains(item: any): boolean;
            toString(): any;
            readonly length: number;
        };
        getChildCount(): number;
        getText(): any;
        toString(): any;
    };
    getChild(i: any, type: any): any;
    getToken(ttype: any, i: any): {} | null;
    getTokens(ttype: any): {}[];
    getTypedRuleContext(ctxType: any, i: any): any;
    getTypedRuleContexts(ctxType: any): any[];
}
/**
 * Used to cache {@link PredictionContext} objects. Its used for the shared
 * context cash associated with contexts in DFA states. This cache
 * can be used for both lexers and parsers.
 */
export class PredictionContextCache {
    cache: {
        data: {};
        hashFunction: any;
        equalsFunction: any;
        set(key: any, value: any): any;
        containsKey(key: any): boolean;
        get(key: any): any;
        entries(): any[];
        getKeys(): any[];
        getValues(): any[];
        toString(): string;
        readonly length: any;
    };
    /**
     * Add a context to the cache and return it. If the context already exists,
     * return that one instead and do not add a new context to the cache.
     * Protect shared cache from unsafe thread access.
     */
    add(ctx: any): any;
    get(ctx: any): any;
    get length(): any;
}
export namespace PredictionMode {
    const SLL: number;
    const LL: number;
    const LL_EXACT_AMBIG_DETECTION: number;
    function hasSLLConflictTerminatingPrediction(mode: any, configs: any): any;
    function hasConfigInRuleStopState(configs: any): any;
    function allConfigsInRuleStopStates(configs: any): any;
    function resolvesToJustOneViableAlt(altsets: any): any;
    function allSubsetsConflict(altsets: any): any;
    function hasNonConflictingAltSet(altsets: any): any;
    function hasConflictingAltSet(altsets: any): any;
    function allSubsetsEqual(altsets: any): any;
    function getUniqueAlt(altsets: any): number;
    function getAlts(altsets: any): {
        data: any[];
        add(value: any): void;
        or(set: any): void;
        remove(value: any): void;
        has(value: any): boolean;
        values(): string[];
        minValue(): number;
        hashCode(): number;
        equals(other: any): boolean;
        toString(): string;
        readonly length: number;
    };
    function getConflictingAltSubsets(configs: any): any[];
    function getStateToAltMap(configs: any): {
        data: {};
        get(key: any): any;
        set(key: any, value: any): void;
        values(): any[];
    };
    function hasStateAssociatedWithOneAlt(configs: any): boolean;
    function getSingleViableAlt(altsets: any): any;
}
/**
 * The root of the ANTLR exception hierarchy. In general, ANTLR tracks just
 *  3 kinds of errors: prediction errors, failed predicate errors, and
 *  mismatched input errors. In each case, the parser knows where it is
 *  in the input, where it is in the ATN, the rule invocation stack,
 *  and what kind of problem occurred.
 */
export class RecognitionException extends Error {
    constructor(params: any);
    message: any;
    recognizer: any;
    input: any;
    ctx: any;
    /**
     * The current {@link Token} when an error occurred. Since not all streams
     * support accessing symbols by index, we have to track the {@link Token}
     * instance itself
    */
    offendingToken: any;
    /**
     * Get the ATN state number the parser was in at the time the error
     * occurred. For {@link NoViableAltException} and
     * {@link LexerNoViableAltException} exceptions, this is the
     * {@link DecisionState} number. For others, it is the state whose outgoing
     * edge we couldn't match.
     */
    offendingState: any;
    /**
     * Gets the set of input symbols which could potentially follow the
     * previously matched symbol at the time this exception was thrown.
     *
     * <p>If the set of expected tokens is not known and could not be computed,
     * this method returns {@code null}.</p>
     *
     * @return The set of token types that could potentially follow the current
     * state in the ATN, or {@code null} if the information is not available.
     */
    getExpectedTokens(): any;
    toString(): any;
}
declare const RuleContext_base: {
    new (): {
        readonly ruleContext: void;
    };
};
export class RuleContext extends RuleContext_base {
    /** A rule context is a record of a single rule invocation. It knows
     * which context invoked it, if any. If there is no parent context, then
     * naturally the invoking state is not valid.  The parent link
     * provides a chain upwards from the current rule invocation to the root
     * of the invocation tree, forming a stack. We actually carry no
     * information about the rule associated with this context (except
     * when parsing). We keep only the state number of the invoking state from
     * the ATN submachine that invoked this. Contrast this with the s
     * pointer inside ParserRuleContext that tracks the current state
     * being "executed" for the current rule.
     *
     * The parent contexts are useful for computing lookahead sets and
     * getting error information.
     *
     * These objects are used during parsing and prediction.
     * For the special case of parsers, we use the subclass
     * ParserRuleContext.
     *
     * @see ParserRuleContext
     */
    constructor(parent: any, invokingState: any);
    parentCtx: any;
    /**
     * What state invoked the rule associated with this context?
     * The "return address" is the followState of invokingState
     * If parent is null, this should be -1.
     */
    invokingState: any;
    depth(): number;
    /**
     * A context is empty if there is no invoking state; meaning nobody call
     * current context.
     */
    isEmpty(): boolean;
    getSourceInterval(): {
        start: any;
        stop: any;
        clone(): any;
        contains(item: any): boolean;
        toString(): any;
        readonly length: number;
    };
    get ruleContext(): {
        parentCtx: any;
        /**
         * What state invoked the rule associated with this context?
         * The "return address" is the followState of invokingState
         * If parent is null, this should be -1.
         */
        invokingState: any;
        depth(): number;
        /**
         * A context is empty if there is no invoking state; meaning nobody call
         * current context.
         */
        isEmpty(): boolean;
        getSourceInterval(): {
            start: any;
            stop: any;
            clone(): any;
            contains(item: any): boolean;
            toString(): any;
            readonly length: number;
        };
        readonly ruleContext: any;
        getPayload(): any;
        /**
         * Return the combined text of all child nodes. This method only considers
         * tokens which have been added to the parse tree.
         * <p>
         * Since tokens on hidden channels (e.g. whitespace or comments) are not
         * added to the parse trees, they will not appear in the output of this
         * method.
         */
        getText(): any;
        /**
         * For rule associated with this parse tree internal node, return
         * the outer alternative number used to match the input. Default
         * implementation does not compute nor store this alt num. Create
         * a subclass of ParserRuleContext with backing field and set
         * option contextSuperClass.
         * to set it.
         */
        getAltNumber(): number;
        /**
         * Set the outer alternative number for this context node. Default
         * implementation does nothing to avoid backing field overhead for
         * trees that don't need it.  Create
         * a subclass of ParserRuleContext with backing field and set
         * option contextSuperClass.
         */
        setAltNumber(altNumber: any): void;
        getChild(i: any): null;
        getChildCount(): number;
        accept(visitor: any): any;
        /**
         * Print out a whole tree, not just a node, in LISP format
         * (root child1 .. childN). Print just a node if this is a leaf.
         */
        toStringTree(ruleNames: any, recog: any): any;
        toString(ruleNames: any, stop: any): string;
    };
    getPayload(): {
        parentCtx: any;
        /**
         * What state invoked the rule associated with this context?
         * The "return address" is the followState of invokingState
         * If parent is null, this should be -1.
         */
        invokingState: any;
        depth(): number;
        /**
         * A context is empty if there is no invoking state; meaning nobody call
         * current context.
         */
        isEmpty(): boolean;
        getSourceInterval(): {
            start: any;
            stop: any;
            clone(): any;
            contains(item: any): boolean;
            toString(): any;
            readonly length: number;
        };
        readonly ruleContext: any;
        getPayload(): any;
        /**
         * Return the combined text of all child nodes. This method only considers
         * tokens which have been added to the parse tree.
         * <p>
         * Since tokens on hidden channels (e.g. whitespace or comments) are not
         * added to the parse trees, they will not appear in the output of this
         * method.
         */
        getText(): any;
        /**
         * For rule associated with this parse tree internal node, return
         * the outer alternative number used to match the input. Default
         * implementation does not compute nor store this alt num. Create
         * a subclass of ParserRuleContext with backing field and set
         * option contextSuperClass.
         * to set it.
         */
        getAltNumber(): number;
        /**
         * Set the outer alternative number for this context node. Default
         * implementation does nothing to avoid backing field overhead for
         * trees that don't need it.  Create
         * a subclass of ParserRuleContext with backing field and set
         * option contextSuperClass.
         */
        setAltNumber(altNumber: any): void;
        getChild(i: any): null;
        getChildCount(): number;
        accept(visitor: any): any;
        /**
         * Print out a whole tree, not just a node, in LISP format
         * (root child1 .. childN). Print just a node if this is a leaf.
         */
        toStringTree(ruleNames: any, recog: any): any;
        toString(ruleNames: any, stop: any): string;
    };
    /**
     * Return the combined text of all child nodes. This method only considers
     * tokens which have been added to the parse tree.
     * <p>
     * Since tokens on hidden channels (e.g. whitespace or comments) are not
     * added to the parse trees, they will not appear in the output of this
     * method.
     */
    getText(): any;
    /**
     * For rule associated with this parse tree internal node, return
     * the outer alternative number used to match the input. Default
     * implementation does not compute nor store this alt num. Create
     * a subclass of ParserRuleContext with backing field and set
     * option contextSuperClass.
     * to set it.
     */
    getAltNumber(): number;
    /**
     * Set the outer alternative number for this context node. Default
     * implementation does nothing to avoid backing field overhead for
     * trees that don't need it.  Create
     * a subclass of ParserRuleContext with backing field and set
     * option contextSuperClass.
     */
    setAltNumber(altNumber: any): void;
    getChild(i: any): null;
    getChildCount(): number;
    accept(visitor: any): any;
    /**
     * Print out a whole tree, not just a node, in LISP format
     * (root child1 .. childN). Print just a node if this is a leaf.
     */
    toStringTree(ruleNames: any, recog: any): any;
    toString(ruleNames: any, stop: any): string;
}
declare const RuleNode_base: {
    new (): {};
};
export class RuleNode extends RuleNode_base {
    get ruleContext(): void;
}
declare const TerminalNode_base: {
    new (): {};
};
export class TerminalNode extends TerminalNode_base {
}
/**
 * A token has properties: text, type, line, character position in the line
 * (so we can ignore tabs), token channel, index, and source from which
 * we obtained this token.
 */
export class Token {
    source: any;
    type: any;
    channel: any;
    start: any;
    stop: any;
    tokenIndex: any;
    line: any;
    column: any;
    _text: any;
    getTokenSource(): any;
    getInputStream(): any;
    set text(arg: any);
    get text(): any;
}
export function arrayToString(a: any): string;
declare namespace index_web {
    export { atn };
    export { dfa };
    export { context };
    export { misc };
    export { tree };
    export { error };
    export { Token };
    export { CommonToken };
    export { InputStream as CharStream };
    export { InputStream };
    export { CommonTokenStream };
    export { Lexer };
    export { Parser };
    export { ParserRuleContext };
    export { Interval };
    export { IntervalSet };
    export { LL1Analyzer };
    export { Utils };
}
declare namespace atn {
    export { ATN };
    export { ATNDeserializer };
    export { LexerATNSimulator };
    export { ParserATNSimulator };
    export { PredictionMode };
    export { PredictionContextCache };
}
declare namespace dfa {
    export { DFA };
    export { DFASerializer };
    export { LexerDFASerializer };
    export { PredPrediction };
}
declare namespace context {
    export { PredictionContext };
}
declare namespace misc {
    export { Interval };
    export { IntervalSet };
}
declare namespace tree {
    export { Trees };
    export { RuleNode };
    export { ErrorNode };
    export { TerminalNode };
    export { ParseTreeListener };
    export { ParseTreeVisitor };
    export { ParseTreeWalker };
}
declare namespace error {
    export { RecognitionException };
    export { NoViableAltException };
    export { LexerNoViableAltException };
    export { InputMismatchException };
    export { FailedPredicateException };
    export { DiagnosticErrorListener };
    export { BailErrorStrategy };
    export { DefaultErrorStrategy };
    export { ErrorListener };
}
declare namespace Utils {
    export { arrayToString };
    export { stringToCharArray };
}
/**
 * A DFA walker that knows how to dump them to serialized strings.
 */
declare class DFASerializer {
    constructor(dfa: any, literalNames: any, symbolicNames: any);
    dfa: any;
    literalNames: any;
    symbolicNames: any;
    toString(): string | null;
    getEdgeLabel(i: any): any;
    getStateString(s: any): string;
}
declare const LexerDFASerializer_base: {
    new (dfa: any, literalNames: any, symbolicNames: any): {
        dfa: any;
        literalNames: any;
        symbolicNames: any;
        toString(): string | null;
        getEdgeLabel(i: any): any;
        getStateString(s: any): string;
    };
};
declare class LexerDFASerializer extends LexerDFASerializer_base {
    constructor(dfa: any);
    getEdgeLabel(i: any): string;
}
/**
 * Map a predicate to a predicted alternative.
 */
declare class PredPrediction {
    constructor(pred: any, alt: any);
    alt: any;
    pred: any;
    toString(): string;
}
declare class PredictionContext {
    constructor(cachedHashCode: any);
    cachedHashCode: any;
    /**
     * Stores the computed hash code of this {@link PredictionContext}. The hash
     * code is computed in parts to match the following reference algorithm.
     *
     * <pre>
     * private int referenceHashCode() {
     * int hash = {@link MurmurHash//initialize MurmurHash.initialize}({@link
     * //INITIAL_HASH});
     *
     * for (int i = 0; i &lt; {@link //size()}; i++) {
     * hash = {@link MurmurHash//update MurmurHash.update}(hash, {@link //getParent
     * getParent}(i));
     * }
     *
     * for (int i = 0; i &lt; {@link //size()}; i++) {
     * hash = {@link MurmurHash//update MurmurHash.update}(hash, {@link
     * //getReturnState getReturnState}(i));
     * }
     *
     * hash = {@link MurmurHash//finish MurmurHash.finish}(hash, 2// {@link
     * //size()});
     * return hash;
     * }
     * </pre>
     * This means only the {@link //EMPTY} context is in set.
     */
    isEmpty(): boolean;
    hasEmptyPath(): boolean;
    hashCode(): any;
    updateHashCode(hash: any): void;
}
declare namespace Trees {
    function toStringTree(tree: any, ruleNames: any, recog: any): any;
    function getNodeText(t: any, ruleNames: any, recog: any): any;
    function getChildren(t: any): any[];
    function getAncestors(t: any): any[];
    function findAllTokenNodes(t: any, ttype: any): any[];
    function findAllRuleNodes(t: any, ruleIndex: any): any[];
    function findAllNodes(t: any, index: any, findTokens: any): any[];
    function _findAllNodes(t: any, index: any, findTokens: any, nodes: any): void;
    function descendants(t: any): any[];
}
declare const ErrorNode_base: {
    new (): {};
};
declare class ErrorNode extends ErrorNode_base {
}
declare const LexerNoViableAltException_base: {
    new (params: any): {
        message: any;
        recognizer: any;
        input: any;
        ctx: any;
        /**
         * The current {@link Token} when an error occurred. Since not all streams
         * support accessing symbols by index, we have to track the {@link Token}
         * instance itself
        */
        offendingToken: any;
        /**
         * Get the ATN state number the parser was in at the time the error
         * occurred. For {@link NoViableAltException} and
         * {@link LexerNoViableAltException} exceptions, this is the
         * {@link DecisionState} number. For others, it is the state whose outgoing
         * edge we couldn't match.
         */
        offendingState: any;
        /**
         * Gets the set of input symbols which could potentially follow the
         * previously matched symbol at the time this exception was thrown.
         *
         * <p>If the set of expected tokens is not known and could not be computed,
         * this method returns {@code null}.</p>
         *
         * @return The set of token types that could potentially follow the current
         * state in the ATN, or {@code null} if the information is not available.
         */
        getExpectedTokens(): any;
        toString(): any;
        name: string;
        stack?: string | undefined;
    };
    captureStackTrace(targetObject: object, constructorOpt?: Function | undefined): void;
    prepareStackTrace?: ((err: Error, stackTraces: NodeJS.CallSite[]) => any) | undefined;
    stackTraceLimit: number;
};
declare class LexerNoViableAltException extends LexerNoViableAltException_base {
    constructor(lexer: any, input: any, startIndex: any, deadEndConfigs: any);
    startIndex: any;
    deadEndConfigs: any;
    toString(): string;
}
declare const InputMismatchException_base: {
    new (params: any): {
        message: any;
        recognizer: any;
        input: any;
        ctx: any;
        /**
         * The current {@link Token} when an error occurred. Since not all streams
         * support accessing symbols by index, we have to track the {@link Token}
         * instance itself
        */
        offendingToken: any;
        /**
         * Get the ATN state number the parser was in at the time the error
         * occurred. For {@link NoViableAltException} and
         * {@link LexerNoViableAltException} exceptions, this is the
         * {@link DecisionState} number. For others, it is the state whose outgoing
         * edge we couldn't match.
         */
        offendingState: any;
        /**
         * Gets the set of input symbols which could potentially follow the
         * previously matched symbol at the time this exception was thrown.
         *
         * <p>If the set of expected tokens is not known and could not be computed,
         * this method returns {@code null}.</p>
         *
         * @return The set of token types that could potentially follow the current
         * state in the ATN, or {@code null} if the information is not available.
         */
        getExpectedTokens(): any;
        toString(): any;
        name: string;
        stack?: string | undefined;
    };
    captureStackTrace(targetObject: object, constructorOpt?: Function | undefined): void;
    prepareStackTrace?: ((err: Error, stackTraces: NodeJS.CallSite[]) => any) | undefined;
    stackTraceLimit: number;
};
/**
 * This signifies any kind of mismatched input exceptions such as
 * when the current input does not match the expected token.
 */
declare class InputMismatchException extends InputMismatchException_base {
}
declare const DefaultErrorStrategy_base: {
    new (): {
        reset(recognizer: any): void;
        recoverInline(recognizer: any): void;
        recover(recognizer: any, e: any): void;
        sync(recognizer: any): void;
        inErrorRecoveryMode(recognizer: any): void;
        reportError(recognizer: any): void;
    };
};
/**
 * This is the default implementation of {@link ANTLRErrorStrategy} used for
 * error reporting and recovery in ANTLR parsers.
 */
declare class DefaultErrorStrategy extends DefaultErrorStrategy_base {
    /**
     * Indicates whether the error strategy is currently "recovering from an
     * error". This is used to suppress reporting multiple error messages while
     * attempting to recover from a detected syntax error.
     *
     * @see //inErrorRecoveryMode
     */
    errorRecoveryMode: boolean;
    /**
     * The index into the input stream where the last error occurred.
     * This is used to prevent infinite loops where an error is found
     * but no token is consumed during recovery...another error is found,
     * ad nauseum. This is a failsafe mechanism to guarantee that at least
     * one token/tree node is consumed for two errors.
     */
    lastErrorIndex: number;
    lastErrorStates: any[] | null;
    nextTokensContext: any;
    nextTokenState: number;
    /**
     * This method is called to enter error recovery mode when a recognition
     * exception is reported.
     *
     * @param recognizer the parser instance
     */
    beginErrorCondition(recognizer: any): void;
    inErrorRecoveryMode(recognizer: any): boolean;
    /**
     * This method is called to leave error recovery mode after recovering from
     * a recognition exception.
     * @param recognizer
     */
    endErrorCondition(recognizer: any): void;
    /**
     * {@inheritDoc}
     * <p>The default implementation simply calls {@link //endErrorCondition}.</p>
     */
    reportMatch(recognizer: any): void;
    /**
     * {@inheritDoc}
     *
     * <p>The default implementation returns immediately if the handler is already
     * in error recovery mode. Otherwise, it calls {@link //beginErrorCondition}
     * and dispatches the reporting task based on the runtime type of {@code e}
     * according to the following table.</p>
     *
     * <ul>
     * <li>{@link NoViableAltException}: Dispatches the call to
     * {@link //reportNoViableAlternative}</li>
     * <li>{@link InputMismatchException}: Dispatches the call to
     * {@link //reportInputMismatch}</li>
     * <li>{@link FailedPredicateException}: Dispatches the call to
     * {@link //reportFailedPredicate}</li>
     * <li>All other types: calls {@link Parser//notifyErrorListeners} to report
     * the exception</li>
     * </ul>
     */
    reportError(recognizer: any, e: any): void;
    nextTokensState: any;
    /**
     * This is called by {@link //reportError} when the exception is a
     * {@link NoViableAltException}.
     *
     * @see //reportError
     *
     * @param recognizer the parser instance
     * @param e the recognition exception
     */
    reportNoViableAlternative(recognizer: any, e: any): void;
    /**
     * This is called by {@link //reportError} when the exception is an
     * {@link InputMismatchException}.
     *
     * @see //reportError
     *
     * @param recognizer the parser instance
     * @param e the recognition exception
     */
    reportInputMismatch(recognizer: any, e: any): void;
    /**
     * This is called by {@link //reportError} when the exception is a
     * {@link FailedPredicateException}.
     *
     * @see //reportError
     *
     * @param recognizer the parser instance
     * @param e the recognition exception
     */
    reportFailedPredicate(recognizer: any, e: any): void;
    /**
     * This method is called to report a syntax error which requires the removal
     * of a token from the input stream. At the time this method is called, the
     * erroneous symbol is current {@code LT(1)} symbol and has not yet been
     * removed from the input stream. When this method returns,
     * {@code recognizer} is in error recovery mode.
     *
     * <p>This method is called when {@link //singleTokenDeletion} identifies
     * single-token deletion as a viable recovery strategy for a mismatched
     * input error.</p>
     *
     * <p>The default implementation simply returns if the handler is already in
     * error recovery mode. Otherwise, it calls {@link //beginErrorCondition} to
     * enter error recovery mode, followed by calling
     * {@link Parser//notifyErrorListeners}.</p>
     *
     * @param recognizer the parser instance
     *
     */
    reportUnwantedToken(recognizer: any): void;
    /**
     * This method is called to report a syntax error which requires the
     * insertion of a missing token into the input stream. At the time this
     * method is called, the missing token has not yet been inserted. When this
     * method returns, {@code recognizer} is in error recovery mode.
     *
     * <p>This method is called when {@link //singleTokenInsertion} identifies
     * single-token insertion as a viable recovery strategy for a mismatched
     * input error.</p>
     *
     * <p>The default implementation simply returns if the handler is already in
     * error recovery mode. Otherwise, it calls {@link //beginErrorCondition} to
     * enter error recovery mode, followed by calling
     * {@link Parser//notifyErrorListeners}.</p>
     *
     * @param recognizer the parser instance
     */
    reportMissingToken(recognizer: any): void;
    /**
     * <p>The default implementation attempts to recover from the mismatched input
     * by using single token insertion and deletion as described below. If the
     * recovery attempt fails, this method throws an
     * {@link InputMismatchException}.</p>
     *
     * <p><strong>EXTRA TOKEN</strong> (single token deletion)</p>
     *
     * <p>{@code LA(1)} is not what we are looking for. If {@code LA(2)} has the
     * right token, however, then assume {@code LA(1)} is some extra spurious
     * token and delete it. Then consume and return the next token (which was
     * the {@code LA(2)} token) as the successful result of the match operation.</p>
     *
     * <p>This recovery strategy is implemented by {@link
        * //singleTokenDeletion}.</p>
     *
     * <p><strong>MISSING TOKEN</strong> (single token insertion)</p>
     *
     * <p>If current token (at {@code LA(1)}) is consistent with what could come
     * after the expected {@code LA(1)} token, then assume the token is missing
     * and use the parser's {@link TokenFactory} to create it on the fly. The
     * "insertion" is performed by returning the created token as the successful
     * result of the match operation.</p>
     *
     * <p>This recovery strategy is implemented by {@link
        * //singleTokenInsertion}.</p>
     *
     * <p><strong>EXAMPLE</strong></p>
     *
     * <p>For example, Input {@code i=(3;} is clearly missing the {@code ')'}. When
     * the parser returns from the nested call to {@code expr}, it will have
     * call chain:</p>
     *
     * <pre>
     * stat &rarr; expr &rarr; atom
     * </pre>
     *
     * and it will be trying to match the {@code ')'} at this point in the
     * derivation:
     *
     * <pre>
     * =&gt; ID '=' '(' INT ')' ('+' atom)* ';'
     * ^
     * </pre>
     *
     * The attempt to match {@code ')'} will fail when it sees {@code ';'} and
     * call {@link //recoverInline}. To recover, it sees that {@code LA(1)==';'}
     * is in the set of tokens that can follow the {@code ')'} token reference
     * in rule {@code atom}. It can assume that you forgot the {@code ')'}.
     */
    recoverInline(recognizer: any): any;
    /**
     * This method implements the single-token insertion inline error recovery
     * strategy. It is called by {@link //recoverInline} if the single-token
     * deletion strategy fails to recover from the mismatched input. If this
     * method returns {@code true}, {@code recognizer} will be in error recovery
     * mode.
     *
     * <p>This method determines whether or not single-token insertion is viable by
     * checking if the {@code LA(1)} input symbol could be successfully matched
     * if it were instead the {@code LA(2)} symbol. If this method returns
     * {@code true}, the caller is responsible for creating and inserting a
     * token with the correct type to produce this behavior.</p>
     *
     * @param recognizer the parser instance
     * @return {@code true} if single-token insertion is a viable recovery
     * strategy for the current mismatched input, otherwise {@code false}
     */
    singleTokenInsertion(recognizer: any): any;
    /**
     * This method implements the single-token deletion inline error recovery
     * strategy. It is called by {@link //recoverInline} to attempt to recover
     * from mismatched input. If this method returns null, the parser and error
     * handler state will not have changed. If this method returns non-null,
     * {@code recognizer} will <em>not</em> be in error recovery mode since the
     * returned token was a successful match.
     *
     * <p>If the single-token deletion is successful, this method calls
     * {@link //reportUnwantedToken} to report the error, followed by
     * {@link Parser//consume} to actually "delete" the extraneous token. Then,
     * before returning {@link //reportMatch} is called to signal a successful
     * match.</p>
     *
     * @param recognizer the parser instance
     * @return the successfully matched {@link Token} instance if single-token
     * deletion successfully recovers from the mismatched input, otherwise
     * {@code null}
     */
    singleTokenDeletion(recognizer: any): any;
    /**
     * Conjure up a missing token during error recovery.
     *
     * The recognizer attempts to recover from single missing
     * symbols. But, actions might refer to that missing symbol.
     * For example, x=ID {f($x);}. The action clearly assumes
     * that there has been an identifier matched previously and that
     * $x points at that token. If that token is missing, but
     * the next token in the stream is what we want we assume that
     * this token is missing and we keep going. Because we
     * have to return some token to replace the missing token,
     * we have to conjure one up. This method gives the user control
     * over the tokens returned for missing tokens. Mostly,
     * you will want to create something special for identifier
     * tokens. For literals such as '{' and ',', the default
     * action in the parser or tree parser works. It simply creates
     * a CommonToken of the appropriate type. The text will be the token.
     * If you change what tokens must be created by the lexer,
     * override this method to create the appropriate tokens.
     *
     */
    getMissingSymbol(recognizer: any): any;
    getExpectedTokens(recognizer: any): any;
    /**
     * How should a token be displayed in an error message? The default
     * is to display just the text, but during development you might
     * want to have a lot of information spit out. Override in that case
     * to use t.toString() (which, for CommonToken, dumps everything about
     * the token). This is better than forcing you to override a method in
     * your token objects because you don't have to go modify your lexer
     * so that it creates a new Java type.
     */
    getTokenErrorDisplay(t: any): string;
    escapeWSAndQuote(s: any): string;
    /**
     * Compute the error recovery set for the current rule. During
     * rule invocation, the parser pushes the set of tokens that can
     * follow that rule reference on the stack; this amounts to
     * computing FIRST of what follows the rule reference in the
     * enclosing rule. See LinearApproximator.FIRST().
     * This local follow set only includes tokens
     * from within the rule; i.e., the FIRST computation done by
     * ANTLR stops at the end of a rule.
     *
     * EXAMPLE
     *
     * When you find a "no viable alt exception", the input is not
     * consistent with any of the alternatives for rule r. The best
     * thing to do is to consume tokens until you see something that
     * can legally follow a call to r//or* any rule that called r.
     * You don't want the exact set of viable next tokens because the
     * input might just be missing a token--you might consume the
     * rest of the input looking for one of the missing tokens.
     *
     * Consider grammar:
     *
     * a : '[' b ']'
     * | '(' b ')'
     * ;
     * b : c '^' INT ;
     * c : ID
     * | INT
     * ;
     *
     * At each rule invocation, the set of tokens that could follow
     * that rule is pushed on a stack. Here are the various
     * context-sensitive follow sets:
     *
     * FOLLOW(b1_in_a) = FIRST(']') = ']'
     * FOLLOW(b2_in_a) = FIRST(')') = ')'
     * FOLLOW(c_in_b) = FIRST('^') = '^'
     *
     * Upon erroneous input "[]", the call chain is
     *
     * a -> b -> c
     *
     * and, hence, the follow context stack is:
     *
     * depth follow set start of rule execution
     * 0 <EOF> a (from main())
     * 1 ']' b
     * 2 '^' c
     *
     * Notice that ')' is not included, because b would have to have
     * been called from a different context in rule a for ')' to be
     * included.
     *
     * For error recovery, we cannot consider FOLLOW(c)
     * (context-sensitive or otherwise). We need the combined set of
     * all context-sensitive FOLLOW sets--the set of all tokens that
     * could follow any reference in the call chain. We need to
     * resync to one of those tokens. Note that FOLLOW(c)='^' and if
     * we resync'd to that token, we'd consume until EOF. We need to
     * sync to context-sensitive FOLLOWs for a, b, and c: {']','^'}.
     * In this case, for input "[]", LA(1) is ']' and in the set, so we would
     * not consume anything. After printing an error, rule c would
     * return normally. Rule b would not find the required '^' though.
     * At this point, it gets a mismatched token error and throws an
     * exception (since LA(1) is not in the viable following token
     * set). The rule exception handler tries to recover, but finds
     * the same recovery set and doesn't consume anything. Rule b
     * exits normally returning to rule a. Now it finds the ']' (and
     * with the successful match exits errorRecovery mode).
     *
     * So, you can see that the parser walks up the call chain looking
     * for the token that was a member of the recovery set.
     *
     * Errors are not generated in errorRecovery mode.
     *
     * ANTLR's error recovery mechanism is based upon original ideas:
     *
     * "Algorithms + Data Structures = Programs" by Niklaus Wirth
     *
     * and
     *
     * "A note on error recovery in recursive descent parsers":
     * http://portal.acm.org/citation.cfm?id=947902.947905
     *
     * Later, Josef Grosch had some good ideas:
     *
     * "Efficient and Comfortable Error Recovery in Recursive Descent
     * Parsers":
     * ftp://www.cocolab.com/products/cocktail/doca4.ps/ell.ps.zip
     *
     * Like Grosch I implement context-sensitive FOLLOW sets that are combined
     * at run-time upon error to avoid overhead during parsing.
     */
    getErrorRecoverySet(recognizer: any): {
        intervals: any[] | null;
        readOnly: boolean;
        first(v: any): any;
        addOne(v: any): void;
        addRange(l: any, h: any): void;
        addInterval(toAdd: any): void;
        addSet(other: any): any;
        reduce(pos: any): void;
        complement(start: any, stop: any): any;
        contains(item: any): boolean;
        removeRange(toRemove: any): void;
        removeOne(value: any): void;
        toString(literalNames: any, symbolicNames: any, elemsAreChar: any): any;
        toCharString(): string;
        toIndexString(): any;
        toTokenString(literalNames: any, symbolicNames: any): any;
        elementName(literalNames: any, symbolicNames: any, token: any): any;
        readonly length: any;
    };
    consumeUntil(recognizer: any, set: any): void;
}
declare function stringToCharArray(str: any): Uint16Array;
export { InputStream as CharStream };
